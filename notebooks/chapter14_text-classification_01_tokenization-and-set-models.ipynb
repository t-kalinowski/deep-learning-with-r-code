{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"dplyr\", \"duckplyr\", \"fs\", \"glue\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "Sys.setenv(\"KERAS_BACKEND\"=\"jax\")\n",
        "library(stringr)\n",
        "library(glue)\n",
        "library(dplyr)\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "library(keras3)\n",
        "library(duckplyr)\n",
        "db_exec(\"SET enable_progress_bar TO false\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "text <- \"The quick brown fox jumped over the lazy dog.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(stringr)\n",
        "library(glue)\n",
        "library(dplyr)\n",
        "library(keras3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_chars <- function(text) {\n",
        "  unlist(str_split(text, boundary(\"character\")))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "\"The quick brown fox jumped over the lazy dog.\" |>\n",
        "  split_chars() |> head(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_words <- function(text) {\n",
        "  text |>\n",
        "    str_split(boundary(\"word\", skip_word_none = FALSE)) |>\n",
        "    unlist() |>\n",
        "    str_subset(\"\\\\S\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_words(\"The quick brown fox jumped over the dog.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "vocabulary <- c(\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"dog\", \".\")\n",
        "words <- split_words(\"The quick brown fox jumped over the lazy dog.\")\n",
        "indices <- match(words, vocabulary, nomatch = 0L)\n",
        "indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A basic character-level tokenizer.\n",
        "new_char_tokenizer <- function(vocabulary, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"CharTokenizer\"\n",
        "\n",
        "  self$vocabulary <- vocabulary\n",
        "  self$nomatch <- nomatch\n",
        "\n",
        "  self$standardize <- function(strings) {\n",
        "    str_to_lower(strings)\n",
        "  }\n",
        "\n",
        "  self$split <- function(strings) {\n",
        "    split_chars(strings)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, self$vocabulary, nomatch = 0L)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(strings) {                                          # <1>\n",
        "    strings |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {                                        # <2>\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- self$vocabulary[indices]\n",
        "    matches[is.na(matches)] <- self$nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing a character-level vocabulary.\n",
        "compute_char_vocabulary <- function(inputs, max_size = Inf) {\n",
        "  tibble(chars = split_chars(inputs)) |>\n",
        "    count(chars, sort = TRUE) |>\n",
        "    slice_head(n = max_size) |>\n",
        "    pull(chars)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A basic word-level tokenizer.\n",
        "new_word_tokenizer <- function(vocabulary, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"WordTokenizer\"\n",
        "\n",
        "  vocabulary; nomatch;                                                          # <1>\n",
        "\n",
        "  self$standardize <- function(string) {\n",
        "    tolower(string)\n",
        "  }\n",
        "\n",
        "  self$split <- function(inputs) {\n",
        "    split_words(inputs)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, vocabulary, nomatch = 0)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(string) {                                           # <2>\n",
        "    string |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {                                        # <3>\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- vocabulary[indices]\n",
        "    matches[is.na(matches)] <- nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing a word-level vocabulary.\n",
        "compute_word_vocabulary <- function(inputs, max_size) {\n",
        "  tibble(words = split_words(inputs)) |>\n",
        "    count(words, sort = TRUE) |>\n",
        "    slice_head(n = max_size) |>\n",
        "    pull(words)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "filename <- get_file(\n",
        "  origin = \"https://www.gutenberg.org/files/2701/old/moby10b.txt\"\n",
        ")\n",
        "moby_dick <- readLines(filename)\n",
        "\n",
        "vocabulary <- compute_char_vocabulary(moby_dick, max_size = 100)\n",
        "char_tokenizer <- new_char_tokenizer(vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(vocabulary)\n",
        "head(vocabulary, 10)\n",
        "tail(vocabulary, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "vocabulary <- compute_word_vocabulary(moby_dick, max_size = 2000)\n",
        "word_tokenizer <- new_word_tokenizer(vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(vocabulary)\n",
        "tail(vocabulary)\n",
        "string <- \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
        "tokenized <- word_tokenizer$tokenize(string)\n",
        "str(tokenized)\n",
        "tokenized |>\n",
        "  word_tokenizer$detokenize() |>\n",
        "  str_flatten(collapse = \" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "data <- c(\n",
        "  \"the quick brown fox\",\n",
        "  \"the slow brown fox\",\n",
        "  \"the quick brown foxhound\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "count_pairs <- function(tokens) {\n",
        "  tibble(left = tokens, right = lead(tokens)) |>\n",
        "    count(left, right, sort = TRUE) |>\n",
        "    filter(left != \" \" & right != \" \")\n",
        "}\n",
        "\n",
        "data |> split_chars() |> count_pairs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hold\n",
        "get_most_common_pair <- function(tokens) {\n",
        "  count_pairs(tokens) |>\n",
        "    slice_max(n, with_ties = FALSE) |>\n",
        "    select(left, right)\n",
        "}\n",
        "\n",
        "merge_pair <- function(tokens, pair) {\n",
        "  matches <- which(\n",
        "    tokens == pair$left & lead(tokens) == pair$right\n",
        "  )\n",
        "\n",
        "  tokens[matches] <- str_c(tokens[matches], tokens[matches + 1])\n",
        "  tokens <- tokens[-(matches + 1)]\n",
        "  tokens\n",
        "}\n",
        "\n",
        "show_tokens <- function(prefix, tokens) {\n",
        "  tokens <- str_flatten(c(\"\", unique(unlist(tokens)), \"\"), collapse = \"_\")\n",
        "  cat(prefix, \": \", tokens, \"\\n\", sep = \"\")\n",
        "}\n",
        "\n",
        "tokens <- data |> split_chars()\n",
        "show_tokens(0, tokens)\n",
        "for (i in seq_len(9)) {\n",
        "  pair <- get_most_common_pair(tokens)\n",
        "  tokens <- tokens |> merge_pair(pair)\n",
        "  show_tokens(i, tokens)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing a byte-pair encoding vocabulary.\n",
        "compute_sub_word_vocabulary <- function(dataset, vocab_size) {\n",
        "  dataset <- split_chars(dataset)\n",
        "  vocab <- compute_char_vocabulary(dataset)\n",
        "  merges <- list()\n",
        "  while (length(vocab) < vocab_size) {\n",
        "    pair <- get_most_common_pair(dataset)\n",
        "    nrow(pair) || break\n",
        "    dataset <- dataset |> merge_pair(pair)\n",
        "    new_token <- str_flatten(pair)\n",
        "    merges[[length(merges) + 1]] <- pair\n",
        "    vocab[[length(vocab) + 1]] <- new_token\n",
        "  }\n",
        "  list(vocab = vocab, merges = merges)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# `append<-` <- c\n",
        "# dataset <- data\n",
        "# compute_sub_word_vocabulary <- function(dataset, vocab_size) {\n",
        "#   # word_counts <- count_and_split_words(dataset)\n",
        "#   dataset <- unlist(split_chars(dataset))\n",
        "#   vocab <- names(sort(table(dataset), decreasing = TRUE))\n",
        "#   merges <- character()\n",
        "#\n",
        "#   while (length(vocab) < vocab_size) {\n",
        "#     counts <- count_pairs(dataset)\n",
        "#     if (!numhash(counts)) break\n",
        "#     .[pair, ..] <- get_most_common_pair(counts)\n",
        "#     dataset <- merge_pair(dataset, pair)\n",
        "#     token <- paste0(pair, collapse = \"\")\n",
        "#     vocab <- c(vocab, token)\n",
        "#     merges <- c(merges, token)\n",
        "#   }\n",
        "#   list(vocab = vocab, merges = merges)\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A byte-pair encoding tokenizer.\n",
        "bpe_merge <- function(data, merges) {\n",
        "  sep <- \"|||SEP|||\"                                                            # <1>\n",
        "  data <- str_flatten(data, collapse = sep)                                     # <1>\n",
        "  for (pair in merges) {                                                        # <2>\n",
        "    .[left, right] <- pair\n",
        "    data <- data |> str_replace_all(                                            # <3>\n",
        "      pattern = fixed(str_c(sep, left, sep, right, sep)),\n",
        "      replacement = str_c(sep, left, right, sep)\n",
        "    )\n",
        "  }\n",
        "  str_split_1(data, fixed(sep))                                                 # <4>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "new_subword_tokenizer <- function(vocabulary, merges, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"SubWordTokenizer\"\n",
        "\n",
        "  vocabulary; merges; nomatch\n",
        "\n",
        "  self$standardize <- function(string) {\n",
        "    tolower(string)\n",
        "  }\n",
        "\n",
        "  self$split <- function(string) {\n",
        "    string |> split_chars() |> bpe_merge(merges)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, vocabulary, nomatch = 0)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(string, nomatch = 0) {\n",
        "    string |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- vocabulary[indices]\n",
        "    matches[is.na(matches)] <- nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# library(stringr)\n",
        "# x <- str_locate_all(moby_dick, fixed(\"th\"))\n",
        "# x\n",
        "# length(unlist(str_extract_all(moby_dick, fixed(\"th\"))))\n",
        "#\n",
        "# sub_word_tokenizer = SubWordTokenizer(vocabulary, merges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "if (!file.exists(\"ch14-bpe-vocab.rds\")) {\n",
        "  .[vocabulary, merges] <- compute_sub_word_vocabulary(moby_dick, 2000)           # <1>\n",
        "  sub_word_tokenizer <- new_subword_tokenizer(vocabulary, merges)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(vocabulary)\n",
        "tail(vocabulary)\n",
        "tokenized <- sub_word_tokenizer$tokenize(string)\n",
        "str(tokenized)\n",
        "tokenized |> sub_word_tokenizer$detokenize() |> str_flatten(\"_\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# glue(r\"---(\n",
        "#   Vocabulary length: { length(vocabulary) }\n",
        "#   Vocabulary head: { str_flatten(double_quote(head(vocabulary)), \" \") }\n",
        "#   Vocabulary tail: { str_flatten(double_quote(tail(vocabulary)), \" \") }\n",
        "#   Line length: { length(word_tokenizer$tokenize(\n",
        "#     \"Call me Ishmael. Some years ago--never mind how long precisely.\")) }\n",
        "#   )---\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Downloading the IMDb movie review dataset.\n",
        "library(keras3)\n",
        "tar_path <- get_file(\n",
        "  origin = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "untar(tar_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "fs::dir_tree(\"aclImdb\", type = \"directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Previewing a single IMDb review.\n",
        "writeLines(strwrap(readLines(\"aclImdb/train/pos/4229_10.txt\", warn = FALSE)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Splitting validation from the IMDb dataset.\n",
        "library(fs)\n",
        "set.seed(1337)\n",
        "base_dir <- path(\"aclImdb\")\n",
        "\n",
        "for (category in c(\"neg\", \"pos\")) {                                             # <1>\n",
        "  filepaths <- dir_ls(base_dir / \"train\" / category)\n",
        "  num_val_samples <- round(0.2 * length(filepaths))\n",
        "\n",
        "  val_files <- sample(filepaths, num_val_samples)\n",
        "\n",
        "  val_dir <- base_dir / \"val\" / category\n",
        "  dir_create(val_dir)\n",
        "  file_move(val_files, val_dir)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading the IMDb dataset for use with Keras.\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n",
        "\n",
        "train_ds <- text_dataset_from_directory(\n",
        "  \"aclImdb/train\",\n",
        "  class_names = c(\"neg\", \"pos\")                                                 # <1>\n",
        ")\n",
        "val_ds <- text_dataset_from_directory(\"aclImdb/val\")\n",
        "test_ds <- text_dataset_from_directory(\"aclImdb/test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[inputs, targets] <- iter_next(as_iterator(train_ds))\n",
        "str(inputs)\n",
        "inputs@r[1]\n",
        "\n",
        "str(targets)\n",
        "targets@r[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Applying a bag-of-words encoding to the IMDb reviews.\n",
        "max_tokens <- 20000\n",
        "text_vectorization <- layer_text_vectorization(\n",
        "  max_tokens = max_tokens,\n",
        "  split = \"whitespace\",                                                         # <1>\n",
        "  output_mode = \"multi_hot\"\n",
        ")\n",
        "\n",
        "train_ds_no_labels <- train_ds |> dataset_map(\\(x, y) x)\n",
        "adapt(text_vectorization, train_ds_no_labels)\n",
        "\n",
        "bag_of_words_train_ds <- train_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "bag_of_words_val_ds <- val_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "bag_of_words_test_ds <- test_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[inputs, targets] <- bag_of_words_train_ds |>\n",
        "  as_array_iterator() |> iter_next()\n",
        "str(inputs)\n",
        "str(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a bag-of-words regression model.\n",
        "build_linear_classifier <- function(max_tokens, name) {\n",
        "  inputs <- keras_input(shape = c(max_tokens))\n",
        "  outputs <- inputs |>\n",
        "    layer_dense(1, activation = \"sigmoid\")\n",
        "  model <- keras_model(inputs, outputs, name = name)\n",
        "  model |> compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = \"accuracy\"\n",
        "  )\n",
        "  model\n",
        "}\n",
        "model <- build_linear_classifier(max_tokens, \"bag_of_words_classifier\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the bag-of-words regression model.\n",
        "early_stopping <- callback_early_stopping(\n",
        "  monitor = \"val_loss\",\n",
        "  restore_best_weights = TRUE,\n",
        "  patience = 2\n",
        ")\n",
        "history <- model |> fit(\n",
        "  bag_of_words_train_ds,\n",
        "  validation_data = bag_of_words_val_ds,\n",
        "  epochs = 10,\n",
        "  callbacks = c(early_stopping)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Training and validation metrics for our bag-of-words model.\n",
        "plot(history, metrics = \"accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Evaluating the bag-of-words regression model.\n",
        "test_result <- evaluate(model, bag_of_words_test_ds)\n",
        "test_result$accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Applying a bigram encoding to the IMDb reviews.\n",
        "max_tokens <- 30000\n",
        "text_vectorization <- layer_text_vectorization(\n",
        "  max_tokens = max_tokens,\n",
        "  split = \"whitespace\",                                                         # <1>\n",
        "  output_mode = \"multi_hot\",\n",
        "  ngrams = 2,                                                                   # <2>\n",
        ")\n",
        "adapt(text_vectorization, train_ds_no_labels)\n",
        "\n",
        "bigram_train_ds <- train_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "bigram_val_ds <- val_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "bigram_test_ds <- test_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[inputs, targets] <- bigram_train_ds |>\n",
        "  as_iterator() |> iter_next()\n",
        "str(inputs)\n",
        "str(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "get_vocabulary(text_vectorization)[100:108]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the bigram regression model.\n",
        "model <- build_linear_classifier(max_tokens, \"bigram_classifier\")\n",
        "model |> fit(\n",
        "  bigram_train_ds,\n",
        "  validation_data = bigram_val_ds,\n",
        "  epochs = 10,\n",
        "  callbacks = early_stopping\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Evaluating the bigram regression model.\n",
        "result <- evaluate(model, bigram_test_ds)\n",
        "result$accuracy\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
