{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "Sys.setenv(\"KERAS_BACKEND\"=\"jax\")\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n",
        "library(stringr)\n",
        "library(keras3)\n",
        "reticulate::py_require(\"keras-hub\")\n",
        "library(tensorflow, exclude = c(\"set_random_seed\", \"shape\"))\n",
        "library(tfdatasets, exclude = \"shape\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "embedding_dim <- 256L\n",
        "hidden_dim <- 1024L\n",
        "max_length <- 250\n",
        "punctuation_regex <- r\"---([!\"#$%&'()*+,./:;<=>?@\\\\^_`{|}~¡¿-])---\"\n",
        "vocab_size <- 15000\n",
        "sequence_length <- 20\n",
        "batch_size <- 64\n",
        "embed_dim <- 256\n",
        "hidden_dim <- 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_input <- function(text, sequence_length) {\n",
        "  starts <- seq.int(1, str_length(text), by = sequence_length)\n",
        "  str_sub(text, cbind(starts, length = sequence_length))\n",
        "}\n",
        "\n",
        "custom_standardization <- function(input_string) {\n",
        "  input_string |>\n",
        "    tf$strings$lower() |>\n",
        "    tf$strings$regex_replace(punctuation_regex, \"\")\n",
        "}\n",
        "\n",
        "format_pair <- function(pair) {\n",
        "  eng <- pair$english |> english_tokenizer()\n",
        "  spa <- pair$spanish |> spanish_tokenizer()\n",
        "\n",
        "  spa_feature <- spa@r[NA:-2]                                                   # <1>\n",
        "  spa_target <- spa@r[2:NA]                                                     # <2>\n",
        "\n",
        "  features <- list(english = eng, spanish = spa_feature)\n",
        "  labels <- spa_target\n",
        "  sample_weight <- labels != 0\n",
        "\n",
        "  tuple(features, labels, sample_weight)\n",
        "}\n",
        "\n",
        "make_dataset <- function(pairs) {\n",
        "  tensor_slices_dataset(pairs) |>\n",
        "    dataset_map(format_pair, num_parallel_calls = 4) |>\n",
        "    dataset_cache() |>\n",
        "    dataset_shuffle(2048) |>\n",
        "    dataset_batch(batch_size) |>\n",
        "    dataset_prefetch(16)\n",
        "}\n",
        "\n",
        "generate_translation <- function(input_sentence) {\n",
        "\n",
        "  tokenized_input_sentence <- english_tokenizer(list(input_sentence))\n",
        "  decoded_sentence <- \"[start]\"\n",
        "\n",
        "  for (i in seq_len(sequence_length)) {\n",
        "    tokenized_target_sentence <- spanish_tokenizer(list(decoded_sentence))\n",
        "    inputs <- list(english = tokenized_input_sentence,\n",
        "                   spanish = tokenized_target_sentence)\n",
        "    next_token_predictions <- predict(seq2seq_rnn, inputs, verbose = 0)\n",
        "    sampled_token_index <- which.max(next_token_predictions[, i, ])\n",
        "    sampled_token <- spa_vocab[sampled_token_index]\n",
        "    decoded_sentence <- str_c(decoded_sentence, \" \", sampled_token)\n",
        "    if (sampled_token == \"[end]\")\n",
        "      break\n",
        "  }\n",
        "  decoded_sentence\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Split marker for notebook/code extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# op_einsum(\"ij->ji\")                                                             # <1>\n",
        "# op_einsum(\"ij,jk->ik\")                                                          # <2>\n",
        "# op_einsum(\"hij,jk->hik\")                                                        # <3>\n",
        "# op_einsum(\"i,i->\")                                                              # <4>\n",
        "# op_einsum(\"ijk,ijk->ijk\")                                                       # <5>\n",
        "# op_einsum(\"ijk,ijk->\")                                                          # <6>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# scores <- sources |>\n",
        "#   sapply(\\(source) score(target, source)) |>\n",
        "#   softmax()\n",
        "# combined <- sum(scores * sources)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# dot_product_attention <- function(target, source) {\n",
        "#   scores <- op_einsum(\"btd,bsd->bts\", target, source)                           # <1>\n",
        "#   scores <- op_softmax(scores, axis = -1)\n",
        "#   op_einsum(\"bts,bsd->btd\", scores, source)                                     # <2>\n",
        "# }\n",
        "#\n",
        "# dot_product_attention(target, source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# query_dense  <- layer_dense(, dim)\n",
        "# key_dense    <- layer_dense(, dim)\n",
        "# value_dense  <- layer_dense(, dim)\n",
        "# output_dense <- layer_dense(, dim)\n",
        "#\n",
        "# parameterized_attention <- function(query, key, value) {\n",
        "#   query <- query |> query_dense()\n",
        "#   key   <- key   |> key_dense()\n",
        "#   value <- value |> value_dense()\n",
        "#\n",
        "#   scores  <-\n",
        "#     op_einsum(\"btd,bsd->bts\", query, key) |>\n",
        "#     op_softmax(axis = -1)\n",
        "#\n",
        "#   outputs <- op_einsum(\"bts,bsd->btd\", scores, value)\n",
        "#   outputs |> output_dense()\n",
        "# }\n",
        "#\n",
        "# parameterized_attention(query = target, key = source, value = source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# query_dense   <- replicate(num_heads, layer_dense(, head_dim))\n",
        "# key_dense     <- replicate(num_heads, layer_dense(, head_dim))\n",
        "# value_dense   <- replicate(num_heads, layer_dense(, head_dim))\n",
        "# output_dense  <- layer_dense(, head_dim * num_heads)\n",
        "#\n",
        "# multi_head_attention <- function(query, key, value) {\n",
        "#   head_outputs <- lapply(seq_len(num_heads), function(i) {\n",
        "#     query <- query |> query_dense[[i]]()\n",
        "#     key   <- key   |> key_dense[[i]]()\n",
        "#     value <- value |> value_dense[[i]]()\n",
        "#\n",
        "#     scores <- op_einsum(\"btd,bsd->bts\", query, key)\n",
        "#     scores <- op_softmax(scores / op_sqrt(head_dim), axis = -1)\n",
        "#     op_einsum(\"bts,bsd->btd\", scores, value)\n",
        "#   })\n",
        "#\n",
        "#   head_outputs |> op_concatenate(axis = -1) |> output_dense()\n",
        "# }\n",
        "#\n",
        "# multi_head_attention(query = target, key = source, value = source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# multi_head_attention <- layer_multi_head_attention(\n",
        "#   num_heads = num_heads,\n",
        "#   head_dim = head_dim\n",
        "# )\n",
        "# multi_head_attention(query = target, key = source, value = source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A Transformer encoder block\n",
        "layer_transformer_encoder <- new_layer_class(\n",
        "  \"TransformerEncoder\",\n",
        "  initialize = function(hidden_dim, intermediate_dim, num_heads) {\n",
        "    super$initialize()\n",
        "    key_dim <- hidden_dim %/% num_heads\n",
        "    self$self_attention <- layer_multi_head_attention(                          # <1>\n",
        "      num_heads = num_heads,\n",
        "      key_dim = key_dim\n",
        "    )\n",
        "    self$self_attention_layernorm <- layer_layer_normalization()                # <1>\n",
        "    self$feed_forward_1 <- layer_dense(, intermediate_dim,                      # <2>\n",
        "                                       activation = \"relu\")                     # <2>\n",
        "    self$feed_forward_2 <- layer_dense(, hidden_dim)                            # <2>\n",
        "    self$feed_forward_layernorm <- layer_layer_normalization()                  # <2>\n",
        "  },\n",
        "  call = function(source, source_mask) {\n",
        "    residual <- x <- source                                                     # <3>\n",
        "    mask <- source_mask@r[, newaxis, ]                                          # <3>\n",
        "    x <- self$self_attention(                                                   # <3>\n",
        "      query = x,                                                                # <3>\n",
        "      key = x,                                                                  # <3>\n",
        "      value = x,                                                                # <3>\n",
        "      attention_mask = mask                                                     # <3>\n",
        "    )                                                                           # <3>\n",
        "    x <- x + residual                                                           # <3>\n",
        "    x <- x |> self$self_attention_layernorm()                                   # <3>\n",
        "\n",
        "    residual <- x                                                               # <4>\n",
        "    x <- x |>                                                                   # <4>\n",
        "      self$feed_forward_1() |>                                                  # <4>\n",
        "      self$feed_forward_2()                                                     # <4>\n",
        "    x <- x + residual                                                           # <4>\n",
        "    x <- x |> self$feed_forward_layernorm()                                     # <4>\n",
        "    x\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "layer_normalization <- function(batch_of_sequences) {\n",
        "  mean <- op_mean(batch_of_sequences, axis = -1, keepdims = TRUE)               # <1>\n",
        "  variance <- op_var(batch_of_sequences, axis = -1, keepdims = TRUE)            # <2>\n",
        "  (batch_of_sequences - mean) / op_sqrt(variance)                               # <2>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "batch_normalization <- function(batch_of_images) {\n",
        "  mean <- op_mean(batch_of_images, axis = c(1, 2, 3), keepdims = TRUE)          # <1>\n",
        "  variance <- op_var(batch_of_images, axis = c(1, 2, 3), keepdims = TRUE)       # <2>\n",
        "  (batch_of_images - mean) / op_sqrt(variance)                                  # <2>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A Transformer decoder block\n",
        "layer_transformer_decoder <- new_layer_class(\n",
        "  \"TransformerDecoder\",\n",
        "  initialize = function(hidden_dim, intermediate_dim, num_heads) {\n",
        "    super$initialize()\n",
        "    key_dim <- hidden_dim %/% num_heads\n",
        "    self$self_attention <- layer_multi_head_attention(\n",
        "      num_heads = num_heads,\n",
        "      key_dim = key_dim\n",
        "    )                                                                           # <1>\n",
        "    self$cross_attention <- layer_multi_head_attention(\n",
        "      num_heads = num_heads,\n",
        "      key_dim = key_dim\n",
        "    )                                                                           # <2>\n",
        "    self$self_attention_layernorm <- layer_layer_normalization()                # <1>\n",
        "    self$cross_attention_layernorm <- layer_layer_normalization()               # <2>\n",
        "    self$feed_forward_1 <- layer_dense(, intermediate_dim,                      # <3>\n",
        "                                       activation = \"relu\")                     # <3>\n",
        "    self$feed_forward_2 <- layer_dense(, hidden_dim)                            # <3>\n",
        "    self$feed_forward_layernorm <- layer_layer_normalization()                  # <3>\n",
        "  },\n",
        "  call = function(target, source, source_mask) {\n",
        "    residual <- x <- target                                                     # <4>\n",
        "    x <- self$self_attention(query = x, key = x, value = x,\n",
        "                             use_causal_mask = TRUE)\n",
        "    x <- x + residual                                                           # <4>\n",
        "    x <- x |> self$self_attention_layernorm()                                   # <4>\n",
        "\n",
        "    residual <- x                                                               # <5>\n",
        "    mask <- source_mask@r[, newaxis, ]                                          # <5>\n",
        "    x <- self$cross_attention(\n",
        "      query = x, key = source, value = source,\n",
        "      attention_mask = mask\n",
        "    )\n",
        "    x <- x + residual                                                           # <5>\n",
        "    x <- x |> self$cross_attention_layernorm()                                  # <5>\n",
        "\n",
        "    residual <- x                                                               # <6>\n",
        "    x <- x |>\n",
        "      self$feed_forward_1() |>\n",
        "      self$feed_forward_2()                                                     # <6>\n",
        "    x <- x + residual                                                           # <6>\n",
        "    x <- x |> self$feed_forward_layernorm()                                     # <6>\n",
        "\n",
        "    x\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "mat <- matrix(0, 5, 5)\n",
        "mat[lower.tri(mat, diag = TRUE)] <- 1\n",
        "write.table(mat, row.names = rep(\" \", 5),\n",
        "            col.names = FALSE, quote = FALSE, sep = \" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a Transformer model\n",
        "hidden_dim <- 256\n",
        "intermediate_dim <- 2048\n",
        "num_heads <- 8\n",
        "\n",
        "encoder <- layer_transformer_encoder(\n",
        "  hidden_dim = hidden_dim,\n",
        "  intermediate_dim = intermediate_dim,\n",
        "  num_heads = num_heads\n",
        ")\n",
        "\n",
        "decoder <- layer_transformer_decoder(\n",
        "  hidden_dim = hidden_dim,\n",
        "  intermediate_dim = intermediate_dim,\n",
        "  num_heads = num_heads\n",
        ")\n",
        "\n",
        "source <- keras_input(shape = NA, dtype = \"int32\", name = \"english\")\n",
        "encoder_output <- source |>\n",
        "  layer_embedding(input_dim = vocab_size, output_dim = hidden_dim) |>\n",
        "  encoder(source_mask = source != 0L)\n",
        "\n",
        "target <- keras_input(shape = list(NULL), dtype = \"int32\", name = \"spanish\")\n",
        "target_predictions <- target |>\n",
        "  layer_embedding(input_dim = vocab_size, output_dim = hidden_dim) |>\n",
        "  decoder(source = encoder_output, source_mask = source != 0L) |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(units = vocab_size, activation = \"softmax\")\n",
        "\n",
        "transformer <- keras_model(\n",
        "  inputs = list(source, target),\n",
        "  outputs = target_predictions\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "transformer |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  weighted_metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "transformer |> fit(train_ds, epochs = 15, validation_data = val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A learned position embedding layer\n",
        "layer_positional_embedding <- new_layer_class(\n",
        "  \"PositionalEmbedding\",\n",
        "  initialize = function(sequence_length, input_dim, output_dim) {\n",
        "    super$initialize()\n",
        "    self$token_embeddings <- layer_embedding(input_dim = input_dim,\n",
        "                                             output_dim = output_dim)\n",
        "    self$position_embeddings <- layer_embedding(input_dim = sequence_length,\n",
        "                                                output_dim = output_dim)\n",
        "  },\n",
        "  call = function(inputs) {\n",
        "    .[.., sequence_length] <- op_shape(inputs)\n",
        "    positions <-\n",
        "      op_arange(0, sequence_length - 1, dtype = \"int32\") |>                     # <1>\n",
        "      op_expand_dims(1)\n",
        "    embedded_tokens <- self$token_embeddings(inputs)\n",
        "    embedded_positions <- self$position_embeddings(positions)\n",
        "    embedded_tokens + embedded_positions\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a Transformer model with position embeddings\n",
        "hidden_dim <- 256\n",
        "intermediate_dim <- 2056\n",
        "num_heads <- 8\n",
        "\n",
        "encoder <- layer_transformer_encoder(\n",
        "  hidden_dim = hidden_dim,\n",
        "  intermediate_dim = intermediate_dim,\n",
        "  num_heads = num_heads\n",
        ")\n",
        "\n",
        "decoder <- layer_transformer_decoder(\n",
        "  hidden_dim = hidden_dim,\n",
        "  intermediate_dim = intermediate_dim,\n",
        "  num_heads = num_heads\n",
        ")\n",
        "\n",
        "source <- keras_input(shape = NA, dtype = \"int32\", name = \"english\")\n",
        "\n",
        "encoder_output <- source |>\n",
        "  layer_positional_embedding(sequence_length, vocab_size, hidden_dim) |>\n",
        "  encoder(source_mask = source != 0L)\n",
        "\n",
        "target <- keras_input(shape = list(NULL), dtype = \"int32\", name = \"spanish\")\n",
        "\n",
        "target_predictions <- target |>\n",
        "  layer_positional_embedding(sequence_length, vocab_size, hidden_dim) |>\n",
        "  decoder(source = encoder_output, source_mask = source != 0L) |>\n",
        "  layer_dropout(rate = 0.5) |>\n",
        "  layer_dense(units = vocab_size, activation = \"softmax\")\n",
        "\n",
        "transformer <- keras_model(\n",
        "  inputs = list(source, target),\n",
        "  outputs = target_predictions\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "transformer |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  weighted_metrics = \"accuracy\"\n",
        ")\n",
        "transformer |> fit(train_ds, epochs = 30, validation_data = val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Generating translations with a Transformer\n",
        "spa_vocab <- get_vocabulary(spanish_tokenizer)\n",
        "\n",
        "generate_translation <- function(input_sentence) {\n",
        "\n",
        "  tokenized_input_sentence <- english_tokenizer(list(input_sentence))\n",
        "  decoded_sentence <- \"[start]\"\n",
        "\n",
        "  for (i in seq_len(sequence_length)) {\n",
        "    tokenized_target_sentence <- spanish_tokenizer(list(decoded_sentence))\n",
        "    tokenized_target_sentence <- tokenized_target_sentence@r[, NA:-2]           # <1>\n",
        "\n",
        "    inputs <- list(english = tokenized_input_sentence,\n",
        "                   spanish = tokenized_target_sentence)\n",
        "    next_token_predictions <- predict(transformer, inputs, verbose = 0)\n",
        "\n",
        "    sampled_token_index <- which.max(next_token_predictions[1, i, ])\n",
        "    sampled_token <- spa_vocab[sampled_token_index]\n",
        "\n",
        "    decoded_sentence <- paste(decoded_sentence, sampled_token)\n",
        "\n",
        "    if (sampled_token == \"[end]\")\n",
        "      break\n",
        "  }\n",
        "\n",
        "  decoded_sentence\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "for (i in sample.int(nrow(test_pairs), 5)) {\n",
        "  input_sentence <- test_pairs$english[i]\n",
        "  writeLines(c(\n",
        "    \"-\",\n",
        "    input_sentence,\n",
        "    generate_translation(input_sentence)\n",
        "  ))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# for (i in sample.int(nrow(test_pairs), 5)) {\n",
        "#   .[english, spanish] <- test_pairs[i, ]\n",
        "#   input_sentence = english\n",
        "#   translated <- generate_translation(english)\n",
        "#   cat(\"-- example\", i, \"--\\n\",\n",
        "#       \"  english:\", english, \"\\n\",\n",
        "#       \"  spanish:\", spanish, \"\\n\",\n",
        "#       \"predicted:\", translated, \"\\n\")\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading the RoBERTa pretrained model with KerasHub\n",
        "py_require(\"keras-hub\")\n",
        "keras_hub <- import(\"keras_hub\")\n",
        "\n",
        "tokenizer <- keras_hub$models$Tokenizer$from_preset(\"roberta_base_en\")\n",
        "backbone <- keras_hub$models$Backbone$from_preset(\"roberta_base_en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "tokenizer(\"The quick brown fox\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "backbone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "batch_size <- 8\n",
        "train_ds <- text_dataset_from_directory(\n",
        "  \"aclImdb/train\", batch_size = batch_size,\n",
        "   class_names = c(\"neg\", \"pos\")\n",
        ")\n",
        "val_ds <- text_dataset_from_directory(\n",
        "  \"aclImdb/val\", batch_size = batch_size\n",
        ")\n",
        "test_ds <- text_dataset_from_directory(\n",
        "  \"aclImdb/test\", batch_size = batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# list(\n",
        "#   c(\"<s>\", \"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\", \"<\/s>\"),\n",
        "#   c(\"<s>\", \"the\", \"panda\", \"slept\", \".\", \"<\/s>\", \"<pad>\", \"<pad>\")\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Preprocessing IMDb movie reviews with RoBERTa’s tokenizer\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "\n",
        "packer <- keras_hub$layers$StartEndPacker(\n",
        "  sequence_length = 512L,\n",
        "  start_value = tokenizer$start_token_id,\n",
        "  end_value = tokenizer$end_token_id,\n",
        "  pad_value = tokenizer$pad_token_id,\n",
        "  return_padding_mask = TRUE,\n",
        ")\n",
        "\n",
        "preprocess <- function(text, label) {\n",
        "  .[token_ids, padding_mask] <- text |> tokenizer() |> packer()\n",
        "  list(\n",
        "    named_list(token_ids, padding_mask),\n",
        "    label\n",
        "  )\n",
        "}\n",
        "\n",
        "preprocessed_train_ds <- train_ds |> dataset_map(preprocess)\n",
        "preprocessed_val_ds   <- val_ds   |> dataset_map(preprocess)\n",
        "preprocessed_test_ds  <- test_ds  |> dataset_map(preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "preprocessed_train_ds |> as_iterator() |> iter_next() |> str()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Extending the base RoBERTa model for classification\n",
        "inputs <- backbone$input\n",
        "outputs <- inputs |>\n",
        "  backbone() |>\n",
        "  op_subset(, 1, ) |>                                                           # <1>\n",
        "  layer_dropout(0.1) |>\n",
        "  layer_dense(768, activation = \"relu\") |>\n",
        "  layer_dropout(0.1) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "classifier <- keras_model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the RoBERTa classification model\n",
        "classifier |> compile(\n",
        "  optimizer = optimizer_adam(5e-5),\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "classifier |> fit(\n",
        "  preprocessed_train_ds,\n",
        "  validation_data = preprocessed_val_ds\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "evaluate(classifier, preprocessed_test_ds)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
