{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(keras3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Adding white-noise channels or all-zeros channels to MNIST\n",
        "library(keras3)\n",
        "\n",
        ".[.[train_images, train_labels], .] <- dataset_mnist()\n",
        "train_images <- array_reshape(train_images / 255, c(60000, 28 * 28))\n",
        "\n",
        "runif_array <- \\(dim) array(runif(prod(dim)), dim)\n",
        "\n",
        "noise_channels <- runif_array(dim(train_images))\n",
        "train_images_with_noise_channels <- cbind(train_images, noise_channels)\n",
        "\n",
        "zeros_channels <- array(0, dim(train_images))\n",
        "train_images_with_zeros_channels <- cbind(train_images, zeros_channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training a model on MNIST data with noise/all-zero channels\n",
        "get_model <- function() {\n",
        "  model <- keras_model_sequential() |>\n",
        "    layer_dense(512, activation = \"relu\") |>\n",
        "    layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "  model |> compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = c(\"accuracy\")\n",
        "  )\n",
        "\n",
        "  model\n",
        "}\n",
        "\n",
        "model <- get_model()\n",
        "history_noise <- model |> fit(\n",
        "  train_images_with_noise_channels, train_labels,\n",
        "  epochs = 10,\n",
        "  batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n",
        "\n",
        "model <- get_model()\n",
        "history_zeros <- model |> fit(\n",
        "  train_images_with_zeros_channels, train_labels,\n",
        "  epochs = 10,\n",
        "  batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Plotting a validation accuracy comparison\n",
        "#| fig-cap: Effect of noise channels on validation accuracy\n",
        "plot(NULL,\n",
        "     main = \"Effect of Noise Channels on Validation Accuracy\",\n",
        "     xlab = \"Epochs\", xlim = c(1, history_noise$params$epochs),\n",
        "     ylab = \"Validation Accuracy\", ylim = c(0.9, 0.98), las = 1)\n",
        "lines(history_zeros$metrics$val_accuracy, lty = 1, type = \"o\")\n",
        "lines(history_noise$metrics$val_accuracy, lty = 2, type = \"o\")\n",
        "legend(\"bottomright\", lty = 1:2,\n",
        "       legend = c(\"Validation accuracy with zeros channels\",\n",
        "                  \"Validation accuracy with noise channels\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Fitting an MNIST model with randomly shuffled labels\n",
        ".[.[train_images, train_labels], .] <- dataset_mnist()\n",
        "train_images <- array_reshape(train_images / 255,\n",
        "                              c(60000, 28 * 28))\n",
        "\n",
        "random_train_labels <- sample(train_labels)                                     # <1>\n",
        "\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(512, activation = \"relu\") |>\n",
        "  layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "model |> compile(optimizer = \"rmsprop\",\n",
        "                 loss = \"sparse_categorical_crossentropy\",\n",
        "                 metrics = \"accuracy\")\n",
        "\n",
        "history <- model |> fit(\n",
        "  train_images, random_train_labels,\n",
        "  epochs = 100, batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "#| lst-cap: Hold-out validation (labels omitted for simplicity)\n",
        "# num_validation_samples <- 10000\n",
        "# val_indices <- sample.int(nrow(data), num_validation_samples)                   # <1>\n",
        "# validation_data <- data[val_indices, ]                                          # <2>\n",
        "# training_data <- data[-val_indices, ]                                           # <3>\n",
        "# model <- get_model()                                                            # <4>\n",
        "# fit(model, training_data, ...)                                                  # <4>\n",
        "# validation_score <- evaluate(model, validation_data, ...)                       # <4>\n",
        "#\n",
        "# ...                                                                             # <5>\n",
        "#\n",
        "# model <- get_model()                                                            # <6>\n",
        "# fit(model, data, ...)                                                           # <6>\n",
        "# test_score <- evaluate(model, test_data, ...)                                   # <6>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "#| lst-cap: K-fold cross-validation (labels omitted for simplicity)\n",
        "# k <- 3\n",
        "# fold_id <- sample(rep(1:k, length.out = nrow(data)))\n",
        "# validation_scores <- numeric(k)\n",
        "#\n",
        "# for (fold in seq_len(k)) {\n",
        "#   validation_idx <- which(fold_id == fold)                                      # <1>\n",
        "#\n",
        "#   validation_data <- data[validation_idx, ]                                     # <1>\n",
        "#   training_data <- data[-validation_idx, ]                                      # <2>\n",
        "#   model <- get_model()                                                          # <3>\n",
        "#   fit(model, training_data, ...)\n",
        "#   validation_score <- evaluate(model, validation_data, ...)\n",
        "#   validation_scores[[fold]] <- validation_score\n",
        "# }\n",
        "#\n",
        "# validation_score <- mean(validation_scores)                                     # <4>\n",
        "# model <- get_model()                                                            # <5>\n",
        "# fit(model, data, ...)                                                           # <5>\n",
        "# test_score <- evaluate(model, test_data, ...)                                   # <5>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training MNIST with an incorrectly high learning rate\n",
        ".[.[train_images, train_labels], .] <- dataset_mnist()\n",
        "train_images <- array_reshape(train_images / 255,\n",
        "                              c(60000, 28 * 28))\n",
        "\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(512, activation = \"relu\") |>\n",
        "  layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = optimizer_rmsprop(1),\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history <- model |> fit(\n",
        "  train_images, train_labels,\n",
        "  epochs = 10, batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: The same model with a more appropriate learning rate\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(512, activation = \"relu\") |>\n",
        "  layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = optimizer_rmsprop(1e-2),\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history <- model |> fit(\n",
        "  train_images, train_labels,\n",
        "  epochs = 10, batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A simple logistic regression on MNIST\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_small_model <- model |> fit(\n",
        "  train_images, train_labels,\n",
        "  epochs = 20,\n",
        "  batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Effect of insufficient model capacity on loss curves\n",
        "plot(history_small_model$metrics$val_loss, type = 'o',\n",
        "     main = \"Effect of Insufficient Model Capacity on Validation Loss\",\n",
        "     xlab = \"Epochs\", ylab = \"Validation Loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model <- keras_model_sequential() |>\n",
        "    layer_dense(128, activation=\"relu\") |>\n",
        "    layer_dense(128, activation=\"relu\") |>\n",
        "    layer_dense(10, activation=\"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer=\"rmsprop\",\n",
        "  loss=\"sparse_categorical_crossentropy\",\n",
        "  metrics=\"accuracy\"\n",
        ")\n",
        "\n",
        "history_large_model <- model |> fit(\n",
        "  train_images, train_labels,\n",
        "  epochs = 20,\n",
        "  batch_size = 128,\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Validation loss for a model with appropriate capacity\n",
        "plot(history_large_model$metrics$val_loss, type = 'o',\n",
        "     main = \"Validation Loss for a Model with Appropriate Capacity\",\n",
        "     xlab = \"Epochs\", ylab = \"Validation Loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(2048, activation = \"relu\") |>\n",
        "  layer_dense(2048, activation = \"relu\") |>\n",
        "  layer_dense(2048, activation = \"relu\") |>\n",
        "  layer_dense(10, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_very_large_model <- model |> fit(\n",
        "  train_images, train_labels,\n",
        "  epochs = 20,\n",
        "  batch_size = 32,                                                              # <1>\n",
        "  validation_split = 0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Effect of excessive model capacity on validation loss\n",
        "plot(history_very_large_model$metrics$val_loss, type = 'o',\n",
        "     main = \"Validation Loss for a Model with Too Much Capacity\",\n",
        "     xlab = \"Epochs\", ylab = \"Validation Loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Original model\n",
        ".[.[train_data, train_labels], .] <- dataset_imdb(num_words = 10000)\n",
        "\n",
        "vectorize_sequences <- function(sequences, dimension = 10000) {\n",
        "  results <- matrix(0, nrow = length(sequences), ncol = dimension)\n",
        "  for (i in seq_along(sequences)) {\n",
        "    idx <- sequences[[i]] + 1L\n",
        "    idx <- idx[idx <= dimension]\n",
        "    results[i, idx] <- 1\n",
        "  }\n",
        "  results\n",
        "}\n",
        "\n",
        "train_data <- vectorize_sequences(train_data)\n",
        "\n",
        "model <- keras_model_sequential() |>\n",
        "    layer_dense(16, activation=\"relu\") |>\n",
        "    layer_dense(16, activation=\"relu\") |>\n",
        "    layer_dense(1, activation=\"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_original <- model |> fit(\n",
        "  train_data, train_labels,\n",
        "  epochs = 20, batch_size = 512, validation_split = 0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Version of the model with lower capacity\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(4, activation = \"relu\") |>\n",
        "  layer_dense(4, activation = \"relu\") |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_smaller_model <- model |> fit(\n",
        "  train_data, train_labels,\n",
        "  epochs = 20, batch_size = 512, validation_split = 0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Original model vs. smaller model on IMDb review classification\n",
        "plot(\n",
        "  NULL,\n",
        "  main = \"Original Model vs. Smaller Model on IMDB Review Classification\",\n",
        "  xlab = \"Epochs\",\n",
        "  xlim = c(1, history_original$params$epochs),\n",
        "  ylab = \"Validation Loss\",\n",
        "  ylim = extendrange(c(history_original$metrics$val_loss,\n",
        "                       history_smaller_model$metrics$val_loss)),\n",
        "  panel.first = abline(v = 1:history_original$params$epochs,\n",
        "                       lty = \"dotted\", col = \"lightgrey\")\n",
        ")\n",
        "\n",
        "lines(history_original     $metrics$val_loss, lty = 2)\n",
        "lines(history_smaller_model$metrics$val_loss, lty = 1)\n",
        "legend(\"topleft\", lty = 2:1,\n",
        "       legend = c(\"Validation loss of original model\",\n",
        "                  \"Validation loss of smaller model\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Version of the model with higher capacity\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(512, activation = \"relu\") |>\n",
        "  layer_dense(512, activation = \"relu\") |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_larger_model <- model |> fit(\n",
        "  train_data, train_labels,\n",
        "  epochs = 20, batch_size = 512, validation_split = 0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Original model vs. much larger model on IMDb review classification\n",
        "plot(\n",
        "  NULL,\n",
        "  main = \"Original Model vs. Much Larger Model on IMDB Review Classification\",\n",
        "  xlab = \"Epochs\", xlim = c(1, history_original$params$epochs),\n",
        "  ylab = \"Validation Loss\",\n",
        "  ylim = range(c(history_original$metrics$val_loss,\n",
        "                 history_larger_model$metrics$val_loss)),\n",
        "  panel.first = abline(v = 1:history_original$params$epochs,\n",
        "                       lty = \"dotted\", col = \"lightgrey\")\n",
        ")\n",
        "lines(history_original    $metrics$val_loss, lty = 2)\n",
        "lines(history_larger_model$metrics$val_loss, lty = 1)\n",
        "legend(\"topleft\", lty = 2:1,\n",
        "       legend = c(\"Validation loss of original model\",\n",
        "                  \"Validation loss of larger model\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Adding L2 weight regularization to the model\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(16, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.002)) |>\n",
        "  layer_dense(16, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.002)) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_l2_reg <- model |> fit(\n",
        "  train_data, train_labels,\n",
        "  epochs = 20, batch_size = 512, validation_split = 0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Effect of L2 weight regularization on validation loss\n",
        "plot(NULL,\n",
        "     main = \"Effect of L2 Weight Regularization on Validation Loss\",\n",
        "     xlab = \"Epochs\", xlim = c(1, history_original$params$epochs),\n",
        "     ylab = \"Validation Loss\",\n",
        "     ylim = range(c(history_original$metrics$val_loss,\n",
        "                    history_l2_reg  $metrics$val_loss)),\n",
        "     panel.first = abline(v = 1:history_original$params$epochs,\n",
        "                          lty = \"dotted\", col = \"lightgrey\"))\n",
        "lines(history_original$metrics$val_loss, lty = 2)\n",
        "lines(history_l2_reg  $metrics$val_loss, lty = 1)\n",
        "legend(\"topleft\", lty = 2:1,\n",
        "       legend = c(\"Validation loss of original model\",\n",
        "                  \"Validation loss of L2-regularized model\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hide\n",
        "#| eval: false\n",
        "#| lst-cap: Weight regularizers available in Keras\n",
        "# regularizer_l1(0.001)                                                           # <1>\n",
        "# regularizer_l1_l2(l1 = 0.001, l2 = 0.001)                                       # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# zero_out <- runif_array(dim(layer_output)) < .5                                 # <1>\n",
        "# layer_output[zero_out] <- 0                                                     # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# layer_output <- layer_output * .5                                               # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# layer_output[runif_array(dim(layer_output)) < dropout_rate] <- 0                # <1>\n",
        "# layer_output <- layer_output / (1 - dropout_rate)                               # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Adding dropout to the IMDb model\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"rmsprop\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "history_dropout <- model |> fit(\n",
        "  train_data, train_labels,\n",
        "  epochs = 20, batch_size = 512,\n",
        "  validation_split = 0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Effect of dropout on validation loss\n",
        "plot(NULL,\n",
        "     main = \"Effect of Dropout on Validation Loss\",\n",
        "     xlab = \"Epochs\", xlim = c(1, history_original$params$epochs),\n",
        "     ylab = \"Validation Loss\",\n",
        "     ylim = range(c(history_original$metrics$val_loss,\n",
        "                    history_dropout $metrics$val_loss)),\n",
        "     panel.first = abline(v = 1:history_original$params$epochs,\n",
        "                          lty = \"dotted\", col = \"lightgrey\"))\n",
        "lines(history_original$metrics$val_loss, lty = 2)\n",
        "lines(history_dropout $metrics$val_loss, lty = 1)\n",
        "legend(\"topleft\", lty = 2:1,\n",
        "       legend = c(\"Validation loss of original model\",\n",
        "                  \"Validation loss of dropout-regularized model\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
