{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"envir\", \"ggplot2\", \"withr\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading the IMDb dataset\n",
        "library(keras3)\n",
        "\n",
        ".[.[train_data, train_labels], .[test_data, test_labels]] <-\n",
        "  dataset_imdb(num_words = 10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# imdb <- dataset_imdb(num_words = 10000)\n",
        "# train_data <- imdb$train$x\n",
        "# train_labels <- imdb$train$y\n",
        "# test_data <- imdb$test$x\n",
        "# test_labels <- imdb$test$y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "max(sapply(train_data, max))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "word_index <- dataset_imdb_word_index()                                         # <1>\n",
        "str(word_index)\n",
        "max(unlist(word_index))\n",
        "stopifnot(all(\n",
        "  1:max(unlist(word_index)) == sort(unlist(word_index))\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Decoding reviews back to text\n",
        "imdb_token_id_to_word <- c(                                                     # <1>\n",
        "  \"<padding>\", \"<start-of-sequence>\", \"<unknown>\", \"<unused>\",                  # <2>\n",
        "  names(sort(unlist(word_index)))                                               # <3>\n",
        ")\n",
        "\n",
        "decode_imdb_words <- function(token_ids) {\n",
        "  paste0(imdb_token_id_to_word[token_ids + 1L],                                 # <4>\n",
        "         collapse = \" \")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "decode_imdb_words(head(train_data[[1]], 32))  |>\n",
        "  strwrap() |> cat(sep = \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Encoding the integer sequences via multi-hot encoding\n",
        "multi_hot_encode <- function(sequences, num_classes) {\n",
        "  results <- matrix(0, nrow = length(sequences), ncol = num_classes)            # <1>\n",
        "  for (i in seq_along(sequences)) {\n",
        "    results[i, sequences[[i]] + 1] <- 1                                         # <2>\n",
        "  }\n",
        "  results\n",
        "}\n",
        "x_train <- multi_hot_encode(train_data, num_classes = 10000)                    # <3>\n",
        "x_test <- multi_hot_encode(test_data, num_classes = 10000)                      # <4>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "y_train <- as.numeric(train_labels)\n",
        "y_test <- as.numeric(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Model definition\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: The sigmoid function\n",
        "sigmoid <- function(x) 1 / (1 + exp(-1 * x))\n",
        "withr::with_par(list(pty = \"s\", las = 1), {\n",
        "  plot(sigmoid, -4, 4,\n",
        "    main = \"Sigmoid\",\n",
        "    ylim = c(-1, 2),\n",
        "    ylab = ~ sigmoid(x), xlab = ~ x,\n",
        "    panel.first = grid())\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: The rectified linear unit function\n",
        "relu <- function(x) pmax(0, x)\n",
        "withr::with_par(list(pty = \"s\", las = 1), {\n",
        "  plot(relu, -4, 4,\n",
        "    main = \"ReLU\",\n",
        "    ylim = c(-1, 2),\n",
        "    ylab = ~ relu(x), xlab = ~ x,\n",
        "    panel.first = grid())\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# output <- dot(input, W) + b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Compiling the model\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = c(\"accuracy\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Setting aside a validation set\n",
        "val_indices <- 1:10000\n",
        "\n",
        "x_val <- x_train[val_indices, ]\n",
        "partial_x_train <- x_train[-val_indices, ]\n",
        "\n",
        "y_val <- y_train[val_indices]\n",
        "partial_y_train <- y_train[-val_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the model\n",
        "history <- model |> fit(\n",
        "  partial_x_train, partial_y_train,\n",
        "  epochs = 20,\n",
        "  batch_size = 512,\n",
        "  validation_data = list(x_val, y_val)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# history <- model |> fit(\n",
        "#   x_train, y_train,\n",
        "#   epochs = 20,\n",
        "#   batch_size = 512,\n",
        "#   validation_split = 0.2\n",
        "# )\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(history$metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Plotting the training and validation loss and accuracy\n",
        "#| fig-cap: IMDb training and validation metrics\n",
        "library(ggplot2)\n",
        "plot(history) + ggtitle(\"[IMDb] Training history\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Retraining a model from scratch\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dense(16, activation = \"relu\") |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "model |> fit(x_train, y_train, epochs = 4, batch_size = 512)\n",
        "\n",
        "results <- model |> evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(results)                                                                    # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "preds <- model |> predict(x_test)\n",
        "str(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Histogram of predicted probabilities (IMDb)\n",
        "hist(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading the Reuters dataset\n",
        ".[.[train_data, train_labels], .[test_data, test_labels]] <-\n",
        "  dataset_reuters(num_words = 10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Decoding newswires back to text\n",
        "word_index <- dataset_reuters_word_index()                                      # <1>\n",
        "reuters_token_id_to_word <- c(                                                  # <2>\n",
        "  \"<padding>\", \"<start-of-sequence>\", \"<unknown>\", \"<unused>\",                  # <3>\n",
        "  names(sort(unlist(word_index)))                                               # <4>\n",
        ")\n",
        "decode_reuters_words <- function(token_ids) {\n",
        "  paste0(reuters_token_id_to_word[token_ids + 1L],                              # <5>\n",
        "         collapse = \" \")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Encoding the input data\n",
        "x_train <- multi_hot_encode(train_data, num_classes = 10000)                    # <1>\n",
        "x_test <- multi_hot_encode(test_data, num_classes = 10000)                      # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Encoding the labels\n",
        "one_hot_encode <- function(labels, num_classes = 46) {\n",
        "  results <- matrix(0, nrow = length(labels), ncol = num_classes)\n",
        "  for (i in seq_along(labels)) {\n",
        "    label_position <- labels[[i]] + 1                                           # <1>\n",
        "    results[i, label_position] <- 1\n",
        "  }\n",
        "  results\n",
        "}\n",
        "\n",
        "y_train <- one_hot_encode(train_labels)                                         # <2>\n",
        "y_test <- one_hot_encode(test_labels)                                           # <3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "y_train <- to_categorical(train_labels)\n",
        "y_test <- to_categorical(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Model definition\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(64, activation = \"relu\") |>\n",
        "  layer_dense(64, activation = \"relu\") |>\n",
        "  layer_dense(46, activation = \"softmax\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Compiling the model\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"categorical_crossentropy\",\n",
        "  metrics = c(\n",
        "    \"accuracy\",\n",
        "    metric_top_k_categorical_accuracy(k = 3, name = \"top_3_accuracy\")\n",
        "  )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Setting aside a validation set\n",
        "val_indices <- 1:1000\n",
        "\n",
        "x_val <- x_train[val_indices,]\n",
        "partial_x_train <- x_train[-val_indices,]\n",
        "\n",
        "y_val <- y_train[val_indices,]\n",
        "partial_y_train <- y_train[-val_indices,]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the model\n",
        "history <- model |> fit(\n",
        "  partial_x_train, partial_y_train,\n",
        "  epochs = 20,\n",
        "  batch_size = 512,\n",
        "  validation_data = list(x_val, y_val)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Plotting the training and validation loss, accuracy, and top-3 accuracy\"\n",
        "#| fig-cap: Training and validation metrics (Reuters)\n",
        "plot(history) + ggtitle(\"Training and validation metrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Retraining a model from scratch\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(64, activation = \"relu\") |>\n",
        "  layer_dense(64, activation = \"relu\") |>\n",
        "  layer_dense(46, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "model |> fit(x_train, y_train, epochs = 9, batch_size = 512)\n",
        "\n",
        "results <- model |> evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "mean(test_labels == sample(test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "predictions <- model |> predict(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "sum(predictions[1, ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "envir::import_from(dplyr, near)\n",
        "all(near(1, rowSums(predictions), tol = 1e-6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "which.max(predictions[1, ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "y_train <- train_labels\n",
        "y_test <- test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: A model with an information bottleneck\n",
        "model <- keras_model_sequential() |>\n",
        "  layer_dense(64, activation = \"relu\") |>\n",
        "  layer_dense(4, activation = \"relu\") |>\n",
        "  layer_dense(46, activation = \"softmax\")\n",
        "\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"categorical_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "history <- model |> fit(\n",
        "  partial_x_train, partial_y_train,\n",
        "  epochs = 20,\n",
        "  batch_size = 128,\n",
        "  validation_data = list(x_val, y_val)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Training history of a model with an information bottleneck\n",
        "plot(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading the California housing dataset\n",
        ".[.[train_data, train_targets], .[test_data, test_targets]] <-\n",
        "  dataset_california_housing(version = \"small\")                                 # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(train_targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Normalizing the data\n",
        "train_mean <- apply(train_data, 2, mean)\n",
        "train_sd <- apply(train_data, 2, sd)\n",
        "x_train <- scale(train_data, center = train_mean, scale = train_sd)\n",
        "x_test <- scale(test_data, center = train_mean, scale = train_sd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Scaling the targets\n",
        "y_train <- train_targets / 100000\n",
        "y_test <- test_targets / 100000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Model definition\n",
        "get_model <- function() {                                                       # <1>\n",
        "  model <- keras_model_sequential() |>\n",
        "    layer_dense(64, activation = \"relu\") |>\n",
        "    layer_dense(64, activation = \"relu\") |>\n",
        "    layer_dense(1)\n",
        "  model |> compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"mean_squared_error\",\n",
        "    metrics = \"mean_absolute_error\"\n",
        "  )\n",
        "  model\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: K-fold validation\n",
        "k <- 4\n",
        "fold_id <- sample(rep(1:k, length.out = nrow(train_data)))\n",
        "num_epochs <- 50\n",
        "all_scores <- numeric(k)\n",
        "\n",
        "for (i in 1:k) {\n",
        "  cat(sprintf(\"Processing fold #%i\\n\", i))\n",
        "\n",
        "  fold_val_indices <- which(fold_id == i)\n",
        "  fold_x_val <- x_train[fold_val_indices, ]                                     # <1>\n",
        "  fold_y_val <- y_train[fold_val_indices]                                       # <1>\n",
        "  fold_x_train <- x_train[-fold_val_indices, ]                                  # <2>\n",
        "  fold_y_train <- y_train[-fold_val_indices]                                    # <2>\n",
        "\n",
        "  model <- get_model()                                                          # <3>\n",
        "  model |> fit(                                                                 # <4>\n",
        "    fold_x_train, fold_y_train,\n",
        "    epochs = num_epochs, batch_size = 16, verbose = 0\n",
        "  )\n",
        "  results <- model |> evaluate(fold_x_val, fold_y_val, verbose = 0)             # <5>\n",
        "  all_scores[i] <- results$mean_absolute_error\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "round(all_scores, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "mean(all_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Saving the validation logs at each fold\n",
        "k <- 4\n",
        "num_epochs <- 200\n",
        "all_mae_histories <- list()\n",
        "\n",
        "for (i in 1:k) {\n",
        "  cat(sprintf(\"Processing fold #%i\\n\", i))\n",
        "\n",
        "  fold_val_indices <- which(fold_id == i)                                       # <1>\n",
        "  fold_x_val <- x_train[fold_val_indices, ]                                     # <1>\n",
        "  fold_y_val <- y_train[fold_val_indices]                                       # <1>\n",
        "  fold_x_train <- x_train[-fold_val_indices, ]                                  # <2>\n",
        "  fold_y_train <- y_train[-fold_val_indices]                                    # <2>\n",
        "\n",
        "  model <- get_model()                                                          # <3>\n",
        "  history <- model |> fit(                                                      # <4>\n",
        "    fold_x_train, fold_y_train,\n",
        "    validation_data = list(fold_x_val, fold_y_val),\n",
        "    epochs = num_epochs, batch_size = 16, verbose = 0\n",
        "  )\n",
        "  mae_history <- history$metrics$val_mean_absolute_error\n",
        "  all_mae_histories[[i]] <- mae_history\n",
        "}\n",
        "\n",
        "all_mae_histories <- do.call(cbind, all_mae_histories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building the history of successive mean K-fold validation scores\n",
        "average_mae_history <- rowMeans(all_mae_histories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Plotting validation scores\n",
        "#| fig-cap: Validation MAE by epoch\n",
        "plot(average_mae_history, ylab = \"Validation MAE\", xlab = \"Epoch\", type = 'l')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Plotting validation scores, excluding the first 10 data points\"\n",
        "#| fig-cap: \"Validation MAE by epoch, excluding the first 10 data points\"\n",
        "truncated_mae_history <- average_mae_history[-(1:10)]\n",
        "plot(average_mae_history, type = 'l',\n",
        "     ylab = \"Validation MAE\", xlab = \"Epoch\",\n",
        "     ylim = range(truncated_mae_history))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the final model\n",
        "model <- get_model()                                                            # <1>\n",
        "model |> fit(x_train, y_train,                                                  # <2>\n",
        "             epochs = 130, batch_size = 16, verbose = 0)\n",
        ".[test_mean_squared_error, test_mean_absolute_error] <-\n",
        "  model |> evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "test_mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "predictions <- model |> predict(x_test)\n",
        "predictions[1, ]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
