{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"dplyr\", \"duckplyr\", \"envir\", \"fs\", \"glue\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "Sys.setenv(\"KERAS_BACKEND\"=\"jax\")\n",
        "library(stringr)\n",
        "library(glue)\n",
        "library(dplyr)\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "library(keras3)\n",
        "library(duckplyr)\n",
        "library(fs)\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "text <- \"The quick brown fox jumped over the lazy dog.\"\n",
        "vocabulary <- c(\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"dog\", \".\")\n",
        "string <- \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
        "max_tokens <- 20000\n",
        "max_tokens <- 30000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_chars <- function(text) {\n",
        "  unlist(str_split(text, boundary(\"character\")))\n",
        "}\n",
        "\n",
        "split_words <- function(text) {\n",
        "  text |>\n",
        "    str_split(boundary(\"word\", skip_word_none = FALSE)) |>\n",
        "    unlist() |>\n",
        "    str_subset(\"\\\\S\")\n",
        "}\n",
        "\n",
        "new_char_tokenizer <- function(vocabulary, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"CharTokenizer\"\n",
        "\n",
        "  self$vocabulary <- vocabulary\n",
        "  self$nomatch <- nomatch\n",
        "\n",
        "  self$standardize <- function(strings) {\n",
        "    str_to_lower(strings)\n",
        "  }\n",
        "\n",
        "  self$split <- function(strings) {\n",
        "    split_chars(strings)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, self$vocabulary, nomatch = 0L)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(strings) {                                          # <1>\n",
        "    strings |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {                                        # <2>\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- self$vocabulary[indices]\n",
        "    matches[is.na(matches)] <- self$nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n",
        "\n",
        "compute_char_vocabulary <- function(inputs, max_size = Inf) {\n",
        "  tibble(chars = split_chars(inputs)) |>\n",
        "    count(chars, sort = TRUE) |>\n",
        "    slice_head(n = max_size) |>\n",
        "    pull(chars)\n",
        "}\n",
        "\n",
        "new_word_tokenizer <- function(vocabulary, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"WordTokenizer\"\n",
        "\n",
        "  vocabulary; nomatch;                                                          # <1>\n",
        "\n",
        "  self$standardize <- function(string) {\n",
        "    tolower(string)\n",
        "  }\n",
        "\n",
        "  self$split <- function(inputs) {\n",
        "    split_words(inputs)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, vocabulary, nomatch = 0)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(string) {                                           # <2>\n",
        "    string |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {                                        # <3>\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- vocabulary[indices]\n",
        "    matches[is.na(matches)] <- nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n",
        "\n",
        "compute_word_vocabulary <- function(inputs, max_size) {\n",
        "  tibble(words = split_words(inputs)) |>\n",
        "    count(words, sort = TRUE) |>\n",
        "    slice_head(n = max_size) |>\n",
        "    pull(words)\n",
        "}\n",
        "\n",
        "count_pairs <- function(tokens) {\n",
        "  tibble(left = tokens, right = lead(tokens)) |>\n",
        "    count(left, right, sort = TRUE) |>\n",
        "    filter(left != \" \" & right != \" \")\n",
        "}\n",
        "\n",
        "get_most_common_pair <- function(tokens) {\n",
        "  count_pairs(tokens) |>\n",
        "    slice_max(n, with_ties = FALSE) |>\n",
        "    select(left, right)\n",
        "}\n",
        "\n",
        "merge_pair <- function(tokens, pair) {\n",
        "  matches <- which(\n",
        "    tokens == pair$left & lead(tokens) == pair$right\n",
        "  )\n",
        "\n",
        "  tokens[matches] <- str_c(tokens[matches], tokens[matches + 1])\n",
        "  tokens <- tokens[-(matches + 1)]\n",
        "  tokens\n",
        "}\n",
        "\n",
        "show_tokens <- function(prefix, tokens) {\n",
        "  tokens <- str_flatten(c(\"\", unique(unlist(tokens)), \"\"), collapse = \"_\")\n",
        "  cat(prefix, \": \", tokens, \"\\n\", sep = \"\")\n",
        "}\n",
        "\n",
        "compute_sub_word_vocabulary <- function(dataset, vocab_size) {\n",
        "  dataset <- split_chars(dataset)\n",
        "  vocab <- compute_char_vocabulary(dataset)\n",
        "  merges <- list()\n",
        "  while (length(vocab) < vocab_size) {\n",
        "    pair <- get_most_common_pair(dataset)\n",
        "    nrow(pair) || break\n",
        "    dataset <- dataset |> merge_pair(pair)\n",
        "    new_token <- str_flatten(pair)\n",
        "    merges[[length(merges) + 1]] <- pair\n",
        "    vocab[[length(vocab) + 1]] <- new_token\n",
        "  }\n",
        "  list(vocab = vocab, merges = merges)\n",
        "}\n",
        "\n",
        "bpe_merge <- function(data, merges) {\n",
        "  sep <- \"|||SEP|||\"                                                            # <1>\n",
        "  data <- str_flatten(data, collapse = sep)                                     # <1>\n",
        "  for (pair in merges) {                                                        # <2>\n",
        "    .[left, right] <- pair\n",
        "    data <- data |> str_replace_all(                                            # <3>\n",
        "      pattern = fixed(str_c(sep, left, sep, right, sep)),\n",
        "      replacement = str_c(sep, left, right, sep)\n",
        "    )\n",
        "  }\n",
        "  str_split_1(data, fixed(sep))                                                 # <4>\n",
        "}\n",
        "\n",
        "new_subword_tokenizer <- function(vocabulary, merges, nomatch = \"[UNK]\") {\n",
        "  self <- new.env(parent = emptyenv())\n",
        "  attr(self, \"class\") <- \"SubWordTokenizer\"\n",
        "\n",
        "  vocabulary; merges; nomatch\n",
        "\n",
        "  self$standardize <- function(string) {\n",
        "    tolower(string)\n",
        "  }\n",
        "\n",
        "  self$split <- function(string) {\n",
        "    string |> split_chars() |> bpe_merge(merges)\n",
        "  }\n",
        "\n",
        "  self$index <- function(tokens) {\n",
        "    match(tokens, vocabulary, nomatch = 0)\n",
        "  }\n",
        "\n",
        "  self$tokenize <- function(string, nomatch = 0) {\n",
        "    string |>\n",
        "      self$standardize() |>\n",
        "      self$split() |>\n",
        "      self$index()\n",
        "  }\n",
        "\n",
        "  self$detokenize <- function(indices) {\n",
        "    indices[indices == 0] <- NA\n",
        "    matches <- vocabulary[indices]\n",
        "    matches[is.na(matches)] <- nomatch\n",
        "    matches\n",
        "  }\n",
        "\n",
        "  self\n",
        "}\n",
        "\n",
        "build_linear_classifier <- function(max_tokens, name) {\n",
        "  inputs <- keras_input(shape = c(max_tokens))\n",
        "  outputs <- inputs |>\n",
        "    layer_dense(1, activation = \"sigmoid\")\n",
        "  model <- keras_model(inputs, outputs, name = name)\n",
        "  model |> compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = \"accuracy\"\n",
        "  )\n",
        "  model\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(fs)\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n",
        "\n",
        "if (!dir_exists(\"aclImdb/train\")) {\n",
        "  stop(\n",
        "    \"Missing directory: 'aclImdb/train'. Run the earlier section that downloads/prepares the IMDb dataset first.\",\n",
        "    call. = FALSE\n",
        "  )\n",
        "}\n",
        "\n",
        "train_ds <- text_dataset_from_directory(\n",
        "  \"aclImdb/train\",\n",
        "  class_names = c(\"neg\", \"pos\")\n",
        ")\n",
        "val_ds <- text_dataset_from_directory(\"aclImdb/val\")\n",
        "test_ds <- text_dataset_from_directory(\"aclImdb/test\")\n",
        "\n",
        "train_ds_no_labels <- train_ds |> dataset_map(\\(x, y) x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Padding IMDb reviews to a fixed sequence length.\n",
        "max_length <- 600\n",
        "max_tokens <- 30000\n",
        "text_vectorization <- layer_text_vectorization(\n",
        "  max_tokens = max_tokens,\n",
        "  split = \"whitespace\",                                                         # <1>\n",
        "  output_mode = \"int\",                                                          # <2>\n",
        "  output_sequence_length = max_length                                           # <3>\n",
        ")\n",
        "text_vectorization |> adapt(train_ds_no_labels)\n",
        "\n",
        "sequence_train_ds <- train_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "sequence_val_ds <- val_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n",
        "sequence_test_ds <- test_ds |>\n",
        "  dataset_map(\\(x, y) tuple(text_vectorization(x), y),\n",
        "              num_parallel_calls = 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[x, y] <- sequence_test_ds |> as_array_iterator() |> iter_next()\n",
        "str(x)\n",
        "tail(x, c(5, 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building an LSTM sequence model.\n",
        "hidden_dim <- 64\n",
        "inputs <- keras_input(shape = c(max_length), dtype = \"int32\")\n",
        "outputs <- inputs |>\n",
        "  op_one_hot(num_classes = max_tokens, zero_indexed = TRUE) |>\n",
        "  layer_bidirectional(layer_lstm(units = hidden_dim)) |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model <- keras_model(inputs, outputs, name = \"lstm_with_one_hot\")\n",
        "model |> compile(optimizer = \"adam\",\n",
        "                 loss = \"binary_crossentropy\",\n",
        "                 metrics = c(\"accuracy\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the LSTM sequence model.\n",
        "model |> fit(\n",
        "  sequence_train_ds,\n",
        "  validation_data = sequence_val_ds,\n",
        "  epochs = 10,\n",
        "  callbacks = c(early_stopping)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Evaluating the LSTM sequence model.\n",
        "evaluate(model, sequence_test_ds)$accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "acc <- evaluate(model, sequence_test_ds)$accuracy\n",
        "acc\n",
        "envir::import_from(scales, percent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Building an LSTM sequence model with an `Embedding` layer.\"\n",
        "hidden_dim <- 64L\n",
        "inputs <- keras_input(shape = c(max_length), dtype = \"int32\")\n",
        "outputs <- inputs |>\n",
        "  layer_embedding(input_dim = max_tokens,\n",
        "                  output_dim = hidden_dim,\n",
        "                  mask_zero = TRUE) |>\n",
        "  layer_bidirectional(layer_lstm(units = hidden_dim)) |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model <- keras_model(inputs, outputs, name = \"lstm_with_embedding\")\n",
        "model |> compile(optimizer = \"adam\",\n",
        "                 loss = \"binary_crossentropy\",\n",
        "                 metrics = \"accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Training and evaluating the LSTM with an `Embedding` layer.\"\n",
        "model |> fit(\n",
        "  sequence_train_ds,\n",
        "  validation_data = sequence_val_ds,\n",
        "  epochs = 10,\n",
        "  callbacks = early_stopping\n",
        ")\n",
        "result <- evaluate(model, sequence_test_ds)\n",
        "result$accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Removing padding from our `TextVectorization` preprocessing layer.\"\n",
        "imdb_vocabulary <- text_vectorization |> get_vocabulary()\n",
        "tokenize_no_padding <- layer_text_vectorization(\n",
        "  vocabulary = imdb_vocabulary,\n",
        "  split = \"whitespace\",\n",
        "  output_mode = \"int\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Preprocessing our IMDb data for pretraining a CBOW model.\n",
        "context_size <- 4L                                                              # <1>\n",
        "window_size <- context_size + 1L + context_size                                 # <2>\n",
        "\n",
        "window_data <- function(token_ids) {\n",
        "  windows <- tf$signal$frame(                                                   # <1>\n",
        "    token_ids,\n",
        "    frame_length = window_size,\n",
        "    frame_step = 1L\n",
        "  )\n",
        "  tensor_slices_dataset(windows)\n",
        "}\n",
        "\n",
        "split_label <- function(window) {\n",
        "  .[left, label, right] <-\n",
        "    tf$split(window, c(context_size, 1L, context_size))\n",
        "  bag <- tf$concat(tuple(left, right), axis = 0L)\n",
        "  tuple(bag, label)\n",
        "}\n",
        "\n",
        "dataset <- text_dataset_from_directory(\"aclImdb/train\", batch_size = NULL)      # <3>\n",
        "\n",
        "dataset <- dataset |>\n",
        "  dataset_map(\\(x, y) x, num_parallel_calls = 8) |>                             # <4>\n",
        "  dataset_map(tokenize_no_padding, num_parallel_calls = 8) |>                   # <5>\n",
        "  dataset_interleave(window_data) |>                                            # <6>\n",
        "  dataset_map(split_label, num_parallel_calls = 8)                              # <7>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a CBOW model.\n",
        "hidden_dim <- 64\n",
        "\n",
        "cbow_embedding <- layer_embedding(\n",
        "  input_dim = max_tokens,\n",
        "  output_dim = hidden_dim\n",
        ")\n",
        "\n",
        "inputs <- keras_input(shape = c(2 * context_size))\n",
        "\n",
        "outputs <- inputs |>\n",
        "  cbow_embedding() |>\n",
        "  layer_global_average_pooling_1d() |>\n",
        "  layer_dense(max_tokens, activation = \"softmax\")\n",
        "\n",
        "cbow_model <- keras_model(inputs, outputs)\n",
        "cbow_model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"sparse_categorical_accuracy\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "cbow_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the CBOW model.\n",
        "dataset <- dataset |> dataset_batch(1024) |> dataset_cache()\n",
        "cbow_model |> fit(dataset, epochs = 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Building another LSTM sequence model with an `Embedding` layer.\"\n",
        "inputs <- keras_input(shape = c(max_length))\n",
        "lstm_embedding <- layer_embedding(\n",
        "  input_dim = max_tokens,\n",
        "  output_dim = hidden_dim,\n",
        "  mask_zero = TRUE\n",
        ")\n",
        "outputs <- inputs |>\n",
        "  lstm_embedding() |>\n",
        "  layer_bidirectional(layer_lstm(units = hidden_dim)) |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(1, activation = \"sigmoid\")\n",
        "\n",
        "model <- keras_model(inputs, outputs, name = \"lstm_with_cbow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hide\n",
        "#| lst-cap: Reusing the CBOW embedding to prime the LSTM model.\n",
        "lstm_embedding$embeddings$assign(cbow_embedding$embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the LSTM model with a pretrained embedding.\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"binary_crossentropy\",\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "model |> fit(\n",
        "  sequence_train_ds,\n",
        "  validation_data = sequence_val_ds,\n",
        "  epochs = 10,\n",
        "  callbacks = c(early_stopping)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Evaluating the LSTM model with a pretrained embedding.\n",
        "result <- evaluate(model, sequence_test_ds)\n",
        "result$accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
