{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"purrr\", \"tibble\", \"viridis\", \"withr\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(keras3)\n",
        "use_backend(\"tensorflow\")\n",
        "reticulate::py_require(\"keras-hub\")\n",
        "py_require(\"keras-hub\")\n",
        "keras_hub <- import(\"keras_hub\")\n",
        "library(tensorflow, exclude = c(\"set_random_seed\", \"shape\"))\n",
        "torch <- import(\"torch\")\n",
        "jax <- import(\"jax\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "get_img_array <- function(img_path, target_size) {\n",
        "  image <- img_path |>\n",
        "    image_load(target_size = target_size) |>                                    # <2>\n",
        "    image_to_array()                                                            # <3>\n",
        "  dim(image) <- c(1, dim(image))                                                # <4>\n",
        "  image\n",
        "}\n",
        "\n",
        "display_image <- function(x, ..., max = 255L, margin = 0) {\n",
        "  par(mar = rep(margin, 4))\n",
        "\n",
        "  x |> as.array() |> drop() |>\n",
        "    as.raster(max = max) |>\n",
        "    plot(..., interpolate = FALSE)\n",
        "}\n",
        "\n",
        "plot_activations <- function(x, ...) {\n",
        "  withr::local_par(list(mar = c(0,0,0,0)))\n",
        "\n",
        "  x <- drop(as.array(x))\n",
        "  if (sum(x) == 0)\n",
        "    return(plot(as.raster(\"gray\")))\n",
        "\n",
        "  rotate <- function(x) t(apply(x, 2, rev))\n",
        "  graphics::image(\n",
        "    rotate(x), asp = 1, axes = FALSE, useRaster = TRUE,\n",
        "    col = viridis::viridis(256), ...\n",
        "  )\n",
        "}\n",
        "\n",
        "compute_loss <- function(image, filter_index) {                                 # <1>\n",
        "  activation <- feature_extractor(image)\n",
        "  filter_activation <- activation@r[, 3:-3, 3:-3, filter_index]                 # <2>\n",
        "  op_mean(filter_activation)                                                    # <3>\n",
        "}\n",
        "\n",
        "gradient_ascent_step <- function(image, filter_index, learning_rate) {\n",
        "  image <- image$clone()$detach()$requires_grad_(TRUE)                          # <1>\n",
        "  loss <- compute_loss(image, filter_index)\n",
        "  loss$backward()\n",
        "  grads <- image$grad\n",
        "  grads <- op_normalize(grads)\n",
        "  image + (learning_rate * grads)\n",
        "}\n",
        "\n",
        "generate_filter_pattern <- function(filter_index) {\n",
        "  iterations <- 30                                                              # <1>\n",
        "  learning_rate <- 10                                                           # <2>\n",
        "  image <- random_uniform(                                                      # <3>\n",
        "    minval = 0.4, maxval = 0.6,                                                 # <3>\n",
        "    shape = shape(1, img_width, img_height, 3)                                  # <3>\n",
        "  )\n",
        "\n",
        "  for (i in seq(iterations))                                                    # <4>\n",
        "    image <- gradient_ascent_step(image, filter_index, learning_rate)           # <4>\n",
        "\n",
        "  image\n",
        "}\n",
        "\n",
        "deprocess_image <- function(image, crop = TRUE) {\n",
        "  image <- op_squeeze(image, axis = 1)                                          # <1>\n",
        "  image <- image - op_mean(image)                                               # <2>\n",
        "  image <- image / op_std(image)                                                # <2>\n",
        "  image <- (image * 64) + 128                                                   # <2>\n",
        "  image <- op_clip(image, 0, 255)                                               # <2>\n",
        "  if (crop) {\n",
        "    image <- image@r[26:-26, 26:-26, ]                                          # <3>\n",
        "  }\n",
        "  op_cast(image, \"uint8\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Split marker for notebook/code extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "img_path <- get_file(                                                           # <1>\n",
        "  fname = \"elephant.jpg\",                                                       # <1>\n",
        "  origin = \"https://img-datasets.s3.amazonaws.com/elephant.jpg\"                 # <1>\n",
        ")                                                                               # <1>\n",
        "img <- img_path |> image_load() |> image_to_array() |> op_expand_dims(1)        # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model <- keras_hub$models$ImageClassifier$from_preset(\n",
        "  \"xception_41_imagenet\",\n",
        "  activation = \"softmax\",                                                       # <1>\n",
        ")\n",
        "preds <- predict(model, img)\n",
        "str(preds)                                                                      # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "decode_imagenet_predictions <- function(preds) {\n",
        "  decoded <- keras_hub$utils$decode_imagenet_predictions(preds)\n",
        "  lapply(decoded, \\(d) {                                                        # <1>\n",
        "    .[class_name, score] <- purrr::list_transpose(d)\n",
        "    tibble::tibble(class_name, score)\n",
        "  })\n",
        "}\n",
        "\n",
        "decode_imagenet_predictions(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "which.max(preds[1, ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "img <- model$preprocessor(img)                                                  # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Returning the last convolutional output\n",
        "last_conv_layer_name <- \"block14_sepconv2_act\"\n",
        "last_conv_layer <- model$backbone$get_layer(last_conv_layer_name)\n",
        "last_conv_layer_model <- keras_model(model$inputs, last_conv_layer$output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Going from the last convolutional output to final predictions\n",
        "classifier_input <- last_conv_layer$output\n",
        "x <- classifier_input\n",
        "for (layer_name in c(\"pooler\", \"predictions\")) {\n",
        "  layer <- model$get_layer(layer_name)\n",
        "  x <- layer(x)\n",
        "}\n",
        "classifier_model <- keras_model(classifier_input, x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing top class gradients with TensorFlow\n",
        "if (keras3::config_backend() == \"tensorflow\") {\n",
        "  tf <- import(\"tensorflow\")\n",
        "  get_top_class_gradients <- function(image_tensor) {\n",
        "    last_conv_layer_output <- last_conv_layer_model(image_tensor)                 # <1>\n",
        "    with(tf$GradientTape() %as% tape, {\n",
        "      tape$watch(last_conv_layer_output)                                          # <1>\n",
        "      preds <- classifier_model(last_conv_layer_output)\n",
        "      top_pred_index <- op_argmax(preds@r[1])\n",
        "      top_class_channel <- preds@r[, top_pred_index]                              # <2>\n",
        "    })\n",
        "\n",
        "    grads <- tape$gradient(top_class_channel, last_conv_layer_output)             # <3>\n",
        "    list(grads, last_conv_layer_output)\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing the top class gradients with PyTorch\n",
        "if (keras3::config_backend() == \"torch\") {\n",
        "  torch <- import(\"torch\")\n",
        "  get_top_class_gradients <- function(image_tensor) {\n",
        "    last_conv_layer_output <- last_conv_layer_model(image_tensor)$                # <1>\n",
        "      clone()$detach()$requires_grad_(TRUE)                                       # <2>\n",
        "\n",
        "    preds <- classifier_model(last_conv_layer_output)                             # <3>\n",
        "    top_pred_index <-  op_argmax(preds@r[1])                                      # <3>\n",
        "    top_class_channel <- preds@r[, top_pred_index]                                # <3>\n",
        "    top_class_channel$backward()                                                  # <4>\n",
        "    grads <- last_conv_layer_output$grad                                          # <4>\n",
        "    list(grads, last_conv_layer_output)\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing the top class gradients with JAX\n",
        "if (keras3::config_backend() == \"jax\") {\n",
        "  jax <- import(\"jax\")\n",
        "\n",
        "  loss_fn <- function(last_conv_layer_output) {                                   # <1>\n",
        "    preds <- classifier_model(last_conv_layer_output)\n",
        "    top_pred_index <- op_argmax(preds@r[1])\n",
        "    top_class_channel <- preds[, top_pred_index]\n",
        "    top_class_channel@r[1]                                                        # <2>\n",
        "  }\n",
        "  grad_fn <- jax$grad(loss_fn)                                                    # <3>\n",
        "\n",
        "  get_top_class_gradients <- function(image_tensor) {\n",
        "    last_conv_layer_output <- last_conv_layer_model(image_tensor)\n",
        "    grads <- -grad_fn(last_conv_layer_output)                                     # <4>\n",
        "    list(grads, last_conv_layer_output)\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Gradient pooling and channel importance weighting\n",
        "img <- img_path |> image_load() |> image_to_array() |> op_expand_dims(1)        # <1>\n",
        "img <- model$preprocessor(img)\n",
        ".[grads, last_conv_layer_output] <- get_top_class_gradients(img)\n",
        "\n",
        "pooled_grads <- op_mean(grads, axis = c(1, 2, 3), keepdims = TRUE)              # <1>\n",
        "output <- last_conv_layer_output * pooled_grads                                 # <2>\n",
        "heatmap <- op_mean(output@r[1], axis = -1)                                      # <3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Standalone class activation heatmap.\n",
        "#| lst-cap: Visualizing the heatmap\n",
        "plot_activations(heatmap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: African elephant class activation heatmap over the test picture\n",
        "#| lst-cap: Superimposing the heatmap on the original picture\n",
        "palette <- hcl.colors(256, palette = \"Spectral\", alpha = .4)                    # <1>\n",
        "heatmap <- as.array(-heatmap)                                                   # <1>\n",
        "heatmap[] <- palette[cut(heatmap, 256)]                                         # <1>\n",
        "heatmap <- as.raster(heatmap)                                                   # <1>\n",
        "\n",
        "img <- image_load(img_path) |> image_to_array()                                 # <2>\n",
        "display_image(img)\n",
        "rasterImage(                                                                    # <3>\n",
        "  heatmap,                                                                      # <3>\n",
        "  0, 0, ncol(img), nrow(img),                                                   # <4>\n",
        "  interpolate = FALSE                                                           # <5>\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
