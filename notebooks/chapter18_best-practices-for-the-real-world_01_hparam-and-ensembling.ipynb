{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"dplyr\", \"tibble\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Sys.setenv(\"XLA_FLAGS\" = \"--xla_force_host_platform_device_count=8\")\n",
        "library(reticulate)\n",
        "library(keras3)\n",
        "use_backend(\"jax\")\n",
        "py_require(c(\"keras-tuner\", \"scikit-learn\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(keras3)\n",
        "library(reticulate)\n",
        "use_backend(\"jax\")\n",
        "py_require(c(\"keras-tuner\", \"scikit-learn\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: KerasTuner model-building function\n",
        "build_model <- function(hp, num_classes = 10) {\n",
        "  units  <- hp$Int(                                                             # <1>\n",
        "    name = \"units\",                                                             # <1>\n",
        "    min_value = 16L,                                                            # <1>\n",
        "    max_value = 64L,                                                            # <1>\n",
        "    step = 16L)                                                                 # <1>\n",
        "  model  <- keras_model_sequential() |>\n",
        "    layer_dense(units, activation = \"relu\") |>\n",
        "    layer_dense(num_classes, activation = \"softmax\")\n",
        "\n",
        "  optimizer <- hp$Choice(name = \"optimizer\",                                    # <2>\n",
        "                         values = c(\"rmsprop\", \"adam\"))                         # <2>\n",
        "  model |> compile(optimizer = optimizer,\n",
        "                   loss = \"sparse_categorical_crossentropy\",\n",
        "                   metrics = \"accuracy\")\n",
        "  model                                                                         # <3>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"KerasTuner `HyperModel`\"\n",
        "kt <- import(\"keras_tuner\")\n",
        "\n",
        "SimpleMLP(kt$HyperModel) %py_class%  {\n",
        "  `__init__` <- function(self, num_classes) {                                   # <1>\n",
        "    self.num_classes = num_classes\n",
        "  }\n",
        "\n",
        "  build <- function(self, hp) {\n",
        "    build_model(hp, self$num_classes)\n",
        "  }\n",
        "}\n",
        "\n",
        "hypermodel <- SimpleMLP(num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "tuner <- kt$BayesianOptimization(\n",
        "  build_model,                                                                  # <1>\n",
        "  objective = \"val_accuracy\",                                                   # <2>\n",
        "  max_trials = 20L,                                                             # <3>\n",
        "  executions_per_trial = 2L,                                                    # <4>\n",
        "  directory = \"mnist_kt_test\",                                                  # <5>\n",
        "  overwrite = TRUE                                                              # <6>\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "tuner$search_space_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[.[x_train_full, y_train_full], .[x_test, y_test]] <- dataset_mnist()          # <1>\n",
        "x_train_full <- x_train_full |> array_reshape(c(-1, 28 * 28))                   # <1>\n",
        "x_train_full <- x_train_full / 255                                              # <1>\n",
        "x_test <- x_test |> array_reshape(c(-1, 28 * 28))                               # <1>\n",
        "x_test <- x_test / 255                                                          # <1>\n",
        "\n",
        "num_val_samples <- 10000                                                        # <2>\n",
        "val_i <- seq_len(num_val_samples)\n",
        "x_val <- x_train_full[val_i, ]                                                  # <2>\n",
        "x_train <- x_train_full[-val_i, ]                                               # <2>\n",
        "y_val <- y_train_full[val_i] |> as.matrix()                                     # <2>\n",
        "y_train <- y_train_full[-val_i] |> as.matrix()                                  # <2>\n",
        "\n",
        "callbacks <- list(                                                              # <3>\n",
        "  callback_early_stopping(monitor = \"val_loss\", patience = 5)                   # <3>\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "tuner$search(                                                                   # <1>\n",
        "  x_train, y_train,\n",
        "  batch_size = 128L,\n",
        "  epochs = 100L,\n",
        "  validation_data = list(x_val, y_val),\n",
        "  callbacks = callbacks,\n",
        "  verbose = 2L\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Querying the best hyperparameter configurations\n",
        "top_n <- 4L\n",
        "best_hps <- tuner$get_best_hyperparameters(top_n)                               # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "get_best_epoch <- function(hp) {\n",
        "  model <- build_model(hp)\n",
        "  callbacks <- list(\n",
        "    callback_early_stopping(\n",
        "      monitor = \"val_loss\",\n",
        "      mode = \"min\",\n",
        "      patience = 10                                                             # <1>\n",
        "    )\n",
        "  )\n",
        "\n",
        "  history <- model |> fit(\n",
        "    x_train, y_train,\n",
        "    validation_data = list(x_val, y_val),\n",
        "    epochs = 100,\n",
        "    batch_size = 128,\n",
        "    callbacks = callbacks\n",
        "  )\n",
        "\n",
        "  best_epoch <- which.min(history$metrics$val_loss)\n",
        "  cat(sprintf(\"Best epoch: %d\\n\", best_epoch))\n",
        "  best_epoch\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "get_best_trained_model <- function(hp) {\n",
        "    best_epoch <- get_best_epoch(hp)\n",
        "    model <- build_model(hp)\n",
        "    model |> fit(\n",
        "        x_train_full, y_train_full,\n",
        "        batch_size=128L, epochs=as.integer(best_epoch * 1.2))\n",
        "    model\n",
        "}\n",
        "\n",
        "best_models <- py_eval(\"[]\", convert = FALSE)\n",
        "for (hp in best_hps) {\n",
        "  model <- get_best_trained_model(hp)\n",
        "  model |> evaluate(x_test, y_test) |> print()\n",
        "  best_models$append(model)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "best_models <- tuner$get_best_models(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# preds_a  <- model_a |> predict(x_val)                                           # <1>\n",
        "# preds_b  <- model_b |> predict(x_val)                                           # <1>\n",
        "# preds_c  <- model_c |> predict(x_val)                                           # <1>\n",
        "# preds_d  <- model_d |> predict(x_val)\n",
        "# final_preds <- 0.25 * (preds_a + preds_b + preds_c + preds_d)                   # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "#\n",
        "# preds_a  <- model_a |> predict(x_val)\n",
        "# preds_b  <- model_b |> predict(x_val)\n",
        "# preds_c  <- model_c |> predict(x_val)\n",
        "# preds_d  <- model_d |> predict(x_val)\n",
        "# final_preds <- .5*preds_a + .25*preds_b + .1*preds_c + .15*preds_d              # <1>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
