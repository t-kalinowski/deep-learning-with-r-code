{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"dplyr\", \"fs\", \"readr\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "Sys.setenv(\"KERAS_BACKEND\"=\"jax\")\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n",
        "library(stringr)\n",
        "library(keras3)\n",
        "reticulate::py_require(\"keras-hub\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "embedding_dim <- 256L\n",
        "hidden_dim <- 1024L\n",
        "max_length <- 250\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "split_input <- function(text, sequence_length) {\n",
        "  starts <- seq.int(1, str_length(text), by = sequence_length)\n",
        "  str_sub(text, cbind(starts, length = sequence_length))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Split marker for notebook/code extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "zip_path <- get_file(\n",
        "  origin = paste0(\n",
        "    \"http://storage.googleapis.com/download.tensorflow.org/\",\n",
        "    \"data/spa-eng.zip\"\n",
        "  ),\n",
        "  extract = TRUE\n",
        ")\n",
        "\n",
        "fs::dir_tree(zip_path, recurse = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "text_path <- fs::path(zip_path, \"spa-eng/spa.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "text_pairs <- text_path |>\n",
        "  readr::read_tsv(col_names = c(\"english\", \"spanish\"),\n",
        "                  col_types = c(\"cc\")) |>\n",
        "  dplyr::mutate(spanish = str_c(\"[start] \", spanish, \" [end]\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "text_pairs\n",
        "text_pairs |> dplyr::slice_sample(n = 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "num_test_samples <- num_val_samples <-\n",
        "  round(0.15 * nrow(text_pairs))\n",
        "num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples\n",
        "\n",
        "pair_group <- sample(c(\n",
        "  rep(\"train\", num_train_samples),\n",
        "  rep(\"test\", num_test_samples),\n",
        "  rep(\"val\", num_val_samples)\n",
        "))\n",
        "\n",
        "train_pairs <- text_pairs[pair_group == \"train\", ]\n",
        "test_pairs <- text_pairs[pair_group == \"test\", ]\n",
        "val_pairs <- text_pairs[pair_group == \"val\", ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Learning token vocabularies for English and Spanish text\n",
        "punctuation_regex <- r\"---([!\"#$%&'()*+,./:;<=>?@\\\\^_`{|}~¡¿-])---\"\n",
        "\n",
        "library(tensorflow, exclude = c(\"set_random_seed\", \"shape\"))\n",
        "custom_standardization <- function(input_string) {\n",
        "  input_string |>\n",
        "    tf$strings$lower() |>\n",
        "    tf$strings$regex_replace(punctuation_regex, \"\")\n",
        "}\n",
        "\n",
        "input_string <- as_tensor(\"[start] ¡corre! [end]\")\n",
        "custom_standardization(input_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "vocab_size <- 15000\n",
        "sequence_length <- 20\n",
        "\n",
        "english_tokenizer <- layer_text_vectorization(\n",
        "  max_tokens = vocab_size,\n",
        "  output_mode = \"int\",\n",
        "  output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "spanish_tokenizer <- layer_text_vectorization(\n",
        "  max_tokens = vocab_size,\n",
        "  output_mode = \"int\",\n",
        "  output_sequence_length = sequence_length + 1,\n",
        "  standardize = custom_standardization\n",
        ")\n",
        "\n",
        "adapt(english_tokenizer, train_pairs$english)\n",
        "adapt(spanish_tokenizer, train_pairs$spanish)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Tokenizing and preparing the translation data\n",
        "format_pair <- function(pair) {\n",
        "  eng <- pair$english |> english_tokenizer()\n",
        "  spa <- pair$spanish |> spanish_tokenizer()\n",
        "\n",
        "  spa_feature <- spa@r[NA:-2]                                                   # <1>\n",
        "  spa_target <- spa@r[2:NA]                                                     # <2>\n",
        "\n",
        "  features <- list(english = eng, spanish = spa_feature)\n",
        "  labels <- spa_target\n",
        "  sample_weight <- labels != 0\n",
        "\n",
        "  tuple(features, labels, sample_weight)\n",
        "}\n",
        "\n",
        "batch_size <- 64\n",
        "\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "make_dataset <- function(pairs) {\n",
        "  tensor_slices_dataset(pairs) |>\n",
        "    dataset_map(format_pair, num_parallel_calls = 4) |>\n",
        "    dataset_cache() |>\n",
        "    dataset_shuffle(2048) |>\n",
        "    dataset_batch(batch_size) |>\n",
        "    dataset_prefetch(16)\n",
        "}\n",
        "\n",
        "train_ds <- make_dataset(train_pairs)\n",
        "val_ds <- make_dataset(val_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        ".[inputs, targets] <- iter_next(as_iterator(train_ds))\n",
        "str(inputs)\n",
        "str(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "inputs <- keras_input(shape = c(sequence_length), dtype = \"int32\")\n",
        "outputs <- inputs |>\n",
        "  layer_embedding(input_dim = vocab_size, output_dim = 128) |>\n",
        "  layer_lstm(32, return_sequences = TRUE) |>\n",
        "  layer_dense(vocab_size, activation = \"softmax\")\n",
        "\n",
        "model <- keras_model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a sequence-to-sequence encoder\n",
        "embed_dim <- 256\n",
        "hidden_dim <- 1024\n",
        "\n",
        "source <- keras_input(shape = c(NA), dtype = \"int32\", name = \"english\")\n",
        "\n",
        "encoder_output <- source |>\n",
        "  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) |>\n",
        "  bidirectional(layer_gru(units = hidden_dim), merge_mode = \"sum\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a sequence-to-sequence decoder\n",
        "target <- keras_input(shape = c(NA), dtype = \"int32\", name = \"spanish\")\n",
        "\n",
        "rnn_layer <- layer_gru(units = hidden_dim, return_sequences = TRUE)\n",
        "\n",
        "target_predictions <- target |>\n",
        "  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) |>\n",
        "  rnn_layer(initial_state = encoder_output) |>\n",
        "  layer_dropout(0.5) |>\n",
        "  layer_dense(vocab_size, activation = \"softmax\")                               # <1>\n",
        "\n",
        "seq2seq_rnn <- keras_model(list(source, target), target_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "seq2seq_rnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "seq2seq_rnn |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  weighted_metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "fit(seq2seq_rnn, train_ds, epochs = 15, validation_data = val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Generating translations with a seq2seq RNN\n",
        "spa_vocab <- spanish_tokenizer$get_vocabulary()\n",
        "\n",
        "generate_translation <- function(input_sentence) {\n",
        "\n",
        "  tokenized_input_sentence <- english_tokenizer(list(input_sentence))\n",
        "  decoded_sentence <- \"[start]\"\n",
        "\n",
        "  for (i in seq_len(sequence_length)) {\n",
        "    tokenized_target_sentence <- spanish_tokenizer(list(decoded_sentence))\n",
        "    inputs <- list(english = tokenized_input_sentence,\n",
        "                   spanish = tokenized_target_sentence)\n",
        "    next_token_predictions <- predict(seq2seq_rnn, inputs, verbose = 0)\n",
        "    sampled_token_index <- which.max(next_token_predictions[, i, ])\n",
        "    sampled_token <- spa_vocab[sampled_token_index]\n",
        "    decoded_sentence <- str_c(decoded_sentence, \" \", sampled_token)\n",
        "    if (sampled_token == \"[end]\")\n",
        "      break\n",
        "  }\n",
        "  decoded_sentence\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "for (i in sample.int(nrow(test_pairs), 5)) {\n",
        "  input_sentence <- test_pairs$english[i]\n",
        "  writeLines(c(\n",
        "    \"-\",\n",
        "    input_sentence,\n",
        "    generate_translation(input_sentence)\n",
        "  ))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# generate_translation(\"Hello\")\n",
        "# generate_translation(\"You know that.\")\n",
        "# generate_translation(\"Thanks.\")\n",
        "# generate_translation(\"You're welcome.\")\n",
        "# generate_translation(\"I think they're happy.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
