{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"dplyr\", \"fs\", \"glue\", \"jsonlite\", \"readr\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(fs)\n",
        "library(stringr)\n",
        "library(keras3)\n",
        "use_backend(\"jax\")\n",
        "py_require(\"keras-hub\")\n",
        "Sys.setenv(\"XLA_PYTHON_CLIENT_MEM_FRACTION\" = \"1.00\")\n",
        "py_require(\"keras_hub\")\n",
        "keras_hub <- import(\"keras_hub\")\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "library(tensorflow, exclude = c(\"shape\", \"set_random_seed\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "batch_size <- 128\n",
        "sequence_length <- 256\n",
        "num_batches <- 29373\n",
        "num_val_batches <- 500\n",
        "hidden_dim <- 512\n",
        "intermediate_dim <- 2056\n",
        "num_heads <- 8\n",
        "num_layers <- 8\n",
        "num_epochs <- 8\n",
        "prompt <- \"A piece of advice\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "read_file <- function(filename) {\n",
        "  text_line_dataset(filename) |>\n",
        "    dataset_map(\\(x) tf$strings$regex_replace(x, \"\\\\\\\\n\", \"\\n\")) |>             # <1>\n",
        "    dataset_map(tokenizer, num_parallel_calls = 8) |>                           # <2>\n",
        "    dataset_map(\\(x) tf$concat(list(x, suffix), -1L))                           # <3>\n",
        "}\n",
        "\n",
        "layer_transformer_decoder <- new_layer_class(\n",
        "  \"TransformerDecoder\",\n",
        "  initialize = function(hidden_dim, intermediate_dim, num_heads) {\n",
        "    super$initialize()\n",
        "    key_dim <- hidden_dim %/% num_heads\n",
        "    self$self_attention <- layer_multi_head_attention(\n",
        "      num_heads = num_heads,\n",
        "      key_dim = key_dim,\n",
        "      dropout = 0.1\n",
        "    )                                                                           # <1>\n",
        "    self$self_attention_layernorm <- layer_layer_normalization()                # <1>\n",
        "    self$feed_forward_1 <- layer_dense(units = intermediate_dim,\n",
        "                                       activation = \"relu\")                     # <2>\n",
        "    self$feed_forward_2 <- layer_dense(units = hidden_dim)                      # <2>\n",
        "    self$feed_forward_layernorm <- layer_layer_normalization()                  # <2>\n",
        "    self$dropout <- layer_dropout(rate = 0.1)                                   # <2>\n",
        "  },\n",
        "  call = function(inputs) {\n",
        "    residual <- x <- inputs                                                     # <3>\n",
        "    x <- self$self_attention(query = x, key = x, value = x,\n",
        "                             use_causal_mask = TRUE)                            # <3>\n",
        "    x <- x |> self$dropout()                                                    # <3>\n",
        "    x <- x + residual                                                           # <3>\n",
        "    x <- x |> self$self_attention_layernorm()                                   # <3>\n",
        "\n",
        "    residual <- x                                                               # <4>\n",
        "    x <- x |>\n",
        "      self$feed_forward_1() |>                                                  # <4>\n",
        "      self$feed_forward_2() |>                                                  # <4>\n",
        "      self$dropout()\n",
        "    x <- x + residual                                                           # <4>\n",
        "    x <- x |> self$feed_forward_layernorm()\n",
        "\n",
        "    x\n",
        "  }\n",
        ")\n",
        "\n",
        "layer_positional_embedding <- new_layer_class(\n",
        "  \"PositionalEmbedding\",\n",
        "  initialize = function(sequence_length, input_dim, output_dim) {\n",
        "    super$initialize()\n",
        "    self$token_embeddings <- layer_embedding(\n",
        "      input_dim = input_dim, output_dim = output_dim\n",
        "    )\n",
        "    self$position_embeddings <- layer_embedding(\n",
        "      input_dim = sequence_length, output_dim = output_dim\n",
        "    )\n",
        "  },\n",
        "  call = function(inputs, reverse = FALSE) {\n",
        "    if (reverse) {\n",
        "      token_embeddings <- self$token_embeddings$embeddings\n",
        "      return(inputs %*% t(token_embeddings))                                    # <1>\n",
        "    }\n",
        "    .[.., sequence_length] <- op_shape(inputs)\n",
        "    positions <-\n",
        "      op_arange(0, sequence_length - 1, dtype = \"int32\") |>\n",
        "      op_expand_dims(1)\n",
        "    embedded_tokens <- self$token_embeddings(inputs)\n",
        "    embedded_positions <- self$position_embeddings(positions)\n",
        "    embedded_tokens + embedded_positions\n",
        "  }\n",
        ")\n",
        "\n",
        "generate <- function(prompt, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  for (i in seq(from = prompt_length + 1, to = max_length)) {\n",
        "    prediction <- mini_gpt(matrix(tokens, nrow = 1))\n",
        "    prediction <- prediction@r[1, -1]\n",
        "    next_token <- op_argmax(prediction, zero_indexed = TRUE)\n",
        "    tokens[i] <- as.array(next_token)\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n",
        "\n",
        "compiled_generate <- function(prompt, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  tokens[seq(prompt_length + 1, max_length)] <- 0L                              # <1>\n",
        "  dim(tokens) <- c(1, max_length)\n",
        "  storage.mode(tokens) <- \"integer\"\n",
        "  for (i in seq(prompt_length, max_length - 1)) {\n",
        "    prediction <- mini_gpt |> predict(tokens, verbose = 0)\n",
        "    prediction <- prediction[, i, ]\n",
        "    next_token <- which.max(prediction) - 1L\n",
        "    tokens[, i + 1] <- next_token\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n",
        "\n",
        "compiled_generate <- function(prompt, sample_fn, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  tokens[seq(prompt_length + 1, max_length)] <- 0L\n",
        "  dim(tokens) <- c(1, max_length)\n",
        "  storage.mode(tokens) <- \"integer\"\n",
        "  for (i in seq(prompt_length, max_length - 1)) {\n",
        "    prediction <- predict(mini_gpt, tokens, verbose = 0)\n",
        "    prediction <- prediction[, i, ]\n",
        "    next_token <- sample_fn(prediction) - 1L\n",
        "    tokens[, i + 1] <- as.array(next_token)\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n",
        "\n",
        "greedy_search <- function(preds) {\n",
        "  op_argmax(preds)\n",
        "}\n",
        "\n",
        "random_sample <- function(preds, temperature = 1) {\n",
        "  preds <- preds / temperature\n",
        "  preds <- op_reshape(preds, c(1, -1))\n",
        "  random_categorical(preds, num_samples = 1) |> op_squeeze()\n",
        "}\n",
        "\n",
        "top_k <- function(preds, k = 5, temperature = 1) {\n",
        "  preds <-  preds / temperature\n",
        "  .[top_preds, top_indices] <- op_top_k(preds, k = k, sorted = FALSE)\n",
        "  choice <- random_sample(top_preds)\n",
        "  op_take(top_indices, choice)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Split marker for notebook/code extraction.\n",
        "#\n",
        "# Ensure the fine-tuning notebook is standalone by (re)loading the base model.\n",
        "keras_hub <- reticulate::import(\"keras_hub\")\n",
        "gemma_lm <- keras_hub$models$CausalLM$from_preset(\"gemma3_1b\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading an instruction fine-tuning dataset\n",
        "library(dplyr, warn.conflicts = FALSE)\n",
        "\n",
        "format_prompt <-\n",
        "  \\(instruction) paste0(\"[instruction]\\n\", instruction, \"[end]\\n[response]\\n\")\n",
        "format_response <-\n",
        "  \\(response) paste0(response, \"[end]\")\n",
        "\n",
        "\n",
        "dataset_path <- get_file(origin = paste0(\n",
        "  \"https://hf.co/datasets/databricks/databricks-dolly-15k/\",\n",
        "  \"resolve/main/databricks-dolly-15k.jsonl\"\n",
        "))\n",
        "\n",
        "data <- readr::read_lines(dataset_path) |>\n",
        "  lapply(jsonlite::parse_json) |>\n",
        "  bind_rows()\n",
        "\n",
        "glimpse(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "data <- data |>\n",
        "  filter(context == \"\") |>\n",
        "  mutate(\n",
        "    prompts = format_prompt(instruction),\n",
        "    responses = format_response(response),\n",
        "    .keep = 'none'\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "str(data[2,])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(tfdatasets, exclude = \"shape\")\n",
        "\n",
        "ds <- data |>\n",
        "  tensor_slices_dataset() |>\n",
        "  dataset_shuffle(2000) |>\n",
        "  dataset_batch(2)\n",
        "\n",
        "val_ds <- ds |> dataset_take(100)\n",
        "train_ds <- ds |> dataset_skip(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "preprocessor <- gemma_lm$preprocessor\n",
        "preprocessor$sequence_length <- 512L\n",
        "batch <- iter_next(as_iterator(train_ds))\n",
        ".[x, y, sample_weight] <- preprocessor(batch)\n",
        "str(x)\n",
        "str(y)\n",
        "str(sample_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# bind\n",
        "rbind(x$token_ids |> as.array() |> _[1, 1:5],\n",
        "      y |> as.array() |> _[1, 1:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "layer_linear <- new_layer_class(\n",
        "  classname = \"Linear\",\n",
        "  initialize = function(input_dim, output_dim) {\n",
        "    super$initialize()\n",
        "    self$kernel <- self$add_weight(shape = shape(input_dim, output_dim))\n",
        "  },\n",
        "  call = function(inputs) {\n",
        "    inputs %*% self$kernel                                                      # <1>\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "layer_lora_linear <- new_layer_class(\n",
        "  classname = \"LoraLinear\",\n",
        "  initialize = function(input_dim, output_dim, rank) {\n",
        "    super$initialize()\n",
        "    self$kernel <- self$add_weight(shape(input_dim, output_dim),\n",
        "                                   trainable = FALSE)\n",
        "    self$alpha <- self$add_weight(shape(input_dim, rank))\n",
        "    self$beta <- self$add_weight(shape(rank, output_dim))\n",
        "  },\n",
        "  call = function(inputs) {\n",
        "    frozen <- inputs %*% self$kernel\n",
        "    update <- inputs %*% self$alpha %*% self$beta\n",
        "    frozen + update\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| lst-cap: Enabling LoRA training for a KerasHub model\n",
        "gemma_lm$backbone$enable_lora(rank = 8L)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# gemma_lm$backbone$trainable <- FALSE                                            # <1>\n",
        "# for (i in seq_len(gemma_lm$backbone$num_layers) - 1) {\n",
        "#   layer <- get_layer(gemma_lm$backbone, sprintf(\"decoder_block_%d\", i))         # <2>\n",
        "#   layer$attention$key_dense$trainable <- TRUE\n",
        "#   layer$attention$key_dense$enable_lora(rank = 8L)\n",
        "#   layer$attention$query_dense$trainable <- TRUE\n",
        "#   layer$attention$query_dense$enable_lora(rank = 8L)\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Fine-tuning a pretrained LLM\n",
        "gemma_lm |> compile(\n",
        "  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n",
        "  optimizer = optimizer_adam(5e-5),\n",
        "  weighted_metrics = metric_sparse_categorical_accuracy()\n",
        ")\n",
        "gemma_lm |> fit(train_ds, validation_data = val_ds, epochs = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$generate(\n",
        "  format_prompt(\"How can I make brownies?\"),\n",
        "  max_length = 512L\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$generate(\n",
        "  format_prompt(\"What is a proper noun?\"),\n",
        "  max_length = 512L\n",
        ") |> cat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$generate(\n",
        "  format_prompt(\"Who is the 542nd president of the United States?\"),\n",
        "  max_length = 512L\n",
        ") |> cat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "#| lst-cap: Pseudocode for the simplest possible RLHF algorithm\n",
        "# for (prompts in dataset) {\n",
        "#   responses <- model$generate(prompts)                                          # <1>\n",
        "#   rewards <- reward_model |> predict(responses)                                 # <2>\n",
        "#   good_responses <- responses[rewards > cutoff]\n",
        "#   model |> fit(good_responses)                                                  # <3>\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "rm(list = setdiff(ls(), \"keras_hub\")); gc();\n",
        "import(\"gc\")$collect(); gc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Loading an instruction-tuned Gemma variant\n",
        "gemma_lm <- keras_hub$models$CausalLM$from_preset(\n",
        "  \"gemma3_instruct_4b\",\n",
        "  dtype = \"float16\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "template_format <- \\(prompt) paste0(\n",
        "  \"<start_of_turn>user\\n\", prompt,\n",
        "  \"<end_of_turn>\\n<start_of_turn>model\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "prompt <- \"Why can't you assign values in Jax tensors? Be brief!\"\n",
        "cat(gemma_lm$generate(template_format(prompt), max_length = 512L))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "prompt <- \"Who is the 542nd president of the United States?\"\n",
        "cat(gemma_lm$generate(template_format(prompt), max_length = 512L))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hide\n",
        "#| fig-cap: An image used to demonstrate multimodal prompting with Gemma.\n",
        "image_url <- paste0(\"https://github.com/mattdangerw/keras-nlp-scripts/\",\n",
        "                    \"blob/main/learned-python.png?raw=true\")\n",
        "image_path <- get_file(origin = image_url)\n",
        "\n",
        "image <- image_path |> image_load() |> image_to_array()\n",
        "par(mar = c(0, 0, 0, 0))\n",
        "plot(as.raster(image, max = 255L))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$preprocessor$max_images_per_prompt <- 1L                               # <1>\n",
        "gemma_lm$preprocessor$sequence_length <- 512L                                   # <1>\n",
        "prompt <- \"What is going on in this image? Be concise!<start_of_image>\"\n",
        "gemma_lm$generate(list(\n",
        "  prompts = template_format(prompt),\n",
        "  images = list(image)\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "prompt = \"What is the snake wearing?<start_of_image>\"\n",
        "gemma_lm$generate(list(\n",
        "  prompts = template_format(prompt),\n",
        "  images = list(image)\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "prompt = glue::trim(r\"(\n",
        "  Judy wrote a 2-page letter to 3 friends twice a week for 3 months.\n",
        "  How many letters did she write?\n",
        "  Be brief, and add \"ANSWER:\" before your final answer.\n",
        "  )\")\n",
        "\n",
        "gemma_lm$compile(sampler = \"random\")                                            # <1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$generate(template_format(prompt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "gemma_lm$generate(template_format(prompt))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
