{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"fs\", \"purrr\", \"readr\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(fs)\n",
        "library(stringr)\n",
        "library(keras3)\n",
        "use_backend(\"jax\")\n",
        "py_require(\"keras-hub\")\n",
        "Sys.setenv(\"XLA_PYTHON_CLIENT_MEM_FRACTION\" = \"1.00\")\n",
        "# config_set_dtype_policy(\"float16\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Downloading a portion of the C4 dataset\n",
        "library(keras3)\n",
        "\n",
        "zipfile <-\n",
        "  \"https://hf.co/datasets/mattdangerw/mini-c4/resolve/main/mini-c4.zip\" |>\n",
        "  get_file(origin = _)\n",
        "\n",
        "unzip(zipfile, exdir = \".\")\n",
        "extract_dir <- fs::path(\"./mini-c4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "library(stringr)\n",
        "\n",
        "fs::path(extract_dir, \"shard0.txt\") |> readLines(n = 1) |>\n",
        "  str_replace_all(r\"(\\\\n)\", \"\\n\") |> str_sub(1, 100) |> cat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Downloading a vocabulary and instantiating a tokenizer\n",
        "py_require(\"keras_hub\")\n",
        "keras_hub <- import(\"keras_hub\")\n",
        "\n",
        "vocabulary_file <- get_file(\n",
        "  origin = \"https://hf.co/mattdangerw/spiece/resolve/main/vocabulary.proto\"\n",
        ")\n",
        "tokenizer <- keras_hub$tokenizers$SentencePieceTokenizer(vocabulary_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "(tokenized <- tokenizer$tokenize(\"The quick brown fox.\"))\n",
        "tokenizer$detokenize(tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# library(fs)\n",
        "# library(readr)\n",
        "# library(purrr)\n",
        "# library(stringr)\n",
        "# library(reticulate)\n",
        "# py_require(\"sentencepiece\")\n",
        "# spm <- import(\"sentencepiece\")\n",
        "#\n",
        "# tmp_dir <- path_temp(\"spm_corpus\")\n",
        "# dir_create(tmp_dir)\n",
        "#\n",
        "# files <- Sys.glob(\"./mini-c4/*.txt\")\n",
        "# new_files <- path(tmp_dir, basename(files))\n",
        "# files <- map2_chr(files, new_files, \\(f, f2) {\n",
        "#   f |>\n",
        "#     read_file() |>\n",
        "#     str_replace_all(r\"(\\\\n)\", \"\\n\") |>\n",
        "#     write_file(f2)\n",
        "#   f2\n",
        "#   }, .progress = TRUE)\n",
        "#\n",
        "# spm$SentencePieceTrainer$train(\n",
        "#   input = files |> str_flatten(\",\"),\n",
        "#   model_prefix = \"mini_gpt\",\n",
        "#   vocab_size = 2048,\n",
        "#   input_sentence_size = 10000,\n",
        "#   max_sentence_length = 50000,\n",
        "#   shuffle_input_sentence = TRUE\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "# mini_tokenizer <-\n",
        "#   keras_hub$tokenizers$SentencePieceTokenizer(\"mini_gpt.model\")\n",
        "# mini_tokenizer$tokenize(\"The quick brown fox.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Preprocessing text input for Transformer pretraining\n",
        "library(tfdatasets, exclude = \"shape\")\n",
        "library(tensorflow, exclude = c(\"shape\", \"set_random_seed\"))\n",
        "\n",
        "batch_size <- 128\n",
        "sequence_length <- 256\n",
        "suffix <- tf$constant(\n",
        "  tokenizer$token_to_id(\"<|endoftext|>\"),\n",
        "  shape = shape(1L)\n",
        ")\n",
        "\n",
        "files <- dir_ls(extract_dir, glob = \"*.txt\")\n",
        "\n",
        "read_file <- function(filename) {\n",
        "  text_line_dataset(filename) |>\n",
        "    dataset_map(\\(x) tf$strings$regex_replace(x, \"\\\\\\\\n\", \"\\n\")) |>             # <1>\n",
        "    dataset_map(tokenizer, num_parallel_calls = 8) |>                           # <2>\n",
        "    dataset_map(\\(x) tf$concat(list(x, suffix), -1L))                           # <3>\n",
        "}\n",
        "\n",
        "\n",
        "ds <- tensor_slices_dataset(files) |>\n",
        "  dataset_interleave(read_file, cycle_length = 32,\n",
        "                     num_parallel_calls = 32) |>                                # <4>\n",
        "  dataset_rebatch(sequence_length + 1, drop_remainder = TRUE)  |>               # <5>\n",
        "  dataset_map(\\(x) list(x@r[NA:-2], x@r[2:NA]),                                 # <6>\n",
        "              num_parallel_calls = 12) |>\n",
        "  dataset_batch(batch_size) |>\n",
        "  dataset_prefetch(buffer_size = 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "num_batches <- 29373\n",
        "num_val_batches <- 500\n",
        "num_train_batches <- num_batches - num_val_batches\n",
        "val_ds <- ds |> dataset_take(num_val_batches) |> dataset_repeat()\n",
        "train_ds <- ds |> dataset_skip(num_val_batches) |> dataset_repeat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Transformer decoder block without cross-attention\n",
        "layer_transformer_decoder <- new_layer_class(\n",
        "  \"TransformerDecoder\",\n",
        "  initialize = function(hidden_dim, intermediate_dim, num_heads) {\n",
        "    super$initialize()\n",
        "    key_dim <- hidden_dim %/% num_heads\n",
        "    self$self_attention <- layer_multi_head_attention(\n",
        "      num_heads = num_heads,\n",
        "      key_dim = key_dim,\n",
        "      dropout = 0.1\n",
        "    )                                                                           # <1>\n",
        "    self$self_attention_layernorm <- layer_layer_normalization()                # <1>\n",
        "    self$feed_forward_1 <- layer_dense(units = intermediate_dim,\n",
        "                                       activation = \"relu\")                     # <2>\n",
        "    self$feed_forward_2 <- layer_dense(units = hidden_dim)                      # <2>\n",
        "    self$feed_forward_layernorm <- layer_layer_normalization()                  # <2>\n",
        "    self$dropout <- layer_dropout(rate = 0.1)                                   # <2>\n",
        "  },\n",
        "  call = function(inputs) {\n",
        "    residual <- x <- inputs                                                     # <3>\n",
        "    x <- self$self_attention(query = x, key = x, value = x,\n",
        "                             use_causal_mask = TRUE)                            # <3>\n",
        "    x <- x |> self$dropout()                                                    # <3>\n",
        "    x <- x + residual                                                           # <3>\n",
        "    x <- x |> self$self_attention_layernorm()                                   # <3>\n",
        "\n",
        "    residual <- x                                                               # <4>\n",
        "    x <- x |>\n",
        "      self$feed_forward_1() |>                                                  # <4>\n",
        "      self$feed_forward_2() |>                                                  # <4>\n",
        "      self$dropout()\n",
        "    x <- x + residual                                                           # <4>\n",
        "    x <- x |> self$feed_forward_layernorm()\n",
        "\n",
        "    x\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Positional embedding layer that can reverse a text embedding\n",
        "layer_positional_embedding <- new_layer_class(\n",
        "  \"PositionalEmbedding\",\n",
        "  initialize = function(sequence_length, input_dim, output_dim) {\n",
        "    super$initialize()\n",
        "    self$token_embeddings <- layer_embedding(\n",
        "      input_dim = input_dim, output_dim = output_dim\n",
        "    )\n",
        "    self$position_embeddings <- layer_embedding(\n",
        "      input_dim = sequence_length, output_dim = output_dim\n",
        "    )\n",
        "  },\n",
        "  call = function(inputs, reverse = FALSE) {\n",
        "    if (reverse) {\n",
        "      token_embeddings <- self$token_embeddings$embeddings\n",
        "      return(inputs %*% t(token_embeddings))                                    # <1>\n",
        "    }\n",
        "    .[.., sequence_length] <- op_shape(inputs)\n",
        "    positions <-\n",
        "      op_arange(0, sequence_length - 1, dtype = \"int32\") |>\n",
        "      op_expand_dims(1)\n",
        "    embedded_tokens <- self$token_embeddings(inputs)\n",
        "    embedded_positions <- self$position_embeddings(positions)\n",
        "    embedded_tokens + embedded_positions\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Creating a mini-GPT functional model\n",
        "config_set_dtype_policy(\"mixed_float16\")                                        # <1>\n",
        "vocab_size <- tokenizer$vocabulary_size()\n",
        "hidden_dim <- 512\n",
        "intermediate_dim <- 2056\n",
        "num_heads <- 8\n",
        "num_layers <- 8\n",
        "\n",
        "inputs <- keras_input(shape = c(NA), dtype = \"int32\", name = \"inputs\")\n",
        "embedding <-\n",
        "  layer_positional_embedding(, sequence_length, vocab_size, hidden_dim)\n",
        "\n",
        "x <- inputs |>\n",
        "  embedding() |>\n",
        "  layer_layer_normalization()\n",
        "\n",
        "for (i in seq_len(num_layers)) {\n",
        "  x <- x |>\n",
        "    layer_transformer_decoder(hidden_dim, intermediate_dim, num_heads)\n",
        "}\n",
        "\n",
        "outputs <- x |> embedding(reverse = TRUE)\n",
        "mini_gpt <- keras_model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Defining a custom learning rate schedule\n",
        "warmup_schedule <- new_learning_rate_schedule_class(\n",
        "  classname = \"WarmupSchedule\",\n",
        "\n",
        "  initialize = function() {\n",
        "    self$rate <- 2e-4                                                           # <1>\n",
        "    self$warmup_steps <- 1000\n",
        "  },\n",
        "\n",
        "  call = function(step) {\n",
        "    step <- step |> op_cast(dtype = \"float32\")\n",
        "    scale <- op_minimum(step / self$warmup_steps, 1)\n",
        "    self$rate * scale\n",
        "  }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| fig-cap: Warmup makes our updates to model parameters smaller at the beginning of training and can help with stability.\n",
        "schedule <- warmup_schedule()\n",
        "x <- seq(0, 5000, by = 10)\n",
        "y <- sapply(x, \\(step) as.array(schedule(step)))\n",
        "\n",
        "par(mar = c(5, 7, 4, 2), bty = \"n\", ann = FALSE)\n",
        "plot(x, y, type = \"l\", lwd = 2, panel.first = grid())\n",
        "title(main = \"Warmup Schedule\", xlab = \"Train Step\")\n",
        "title(ylab = \"Learning Rate\", line = 4.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training the mini-GPT model\n",
        "num_epochs <- 8\n",
        "steps_per_epoch <- num_train_batches %/% num_epochs                             # <1>\n",
        "validation_steps <- num_val_batches                                             # <1>\n",
        "\n",
        "mini_gpt |> compile(\n",
        "  optimizer = optimizer_adam(schedule),\n",
        "  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n",
        "  metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "mini_gpt |> fit(\n",
        "  train_ds,\n",
        "  validation_data = val_ds,\n",
        "  epochs = num_epochs,\n",
        "  steps_per_epoch = steps_per_epoch,\n",
        "  validation_steps = validation_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Simple generation function for the mini-GPT model\n",
        "generate <- function(prompt, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  for (i in seq(from = prompt_length + 1, to = max_length)) {\n",
        "    prediction <- mini_gpt(matrix(tokens, nrow = 1))\n",
        "    prediction <- prediction@r[1, -1]\n",
        "    next_token <- op_argmax(prediction, zero_indexed = TRUE)\n",
        "    tokens[i] <- as.array(next_token)\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "prompt <- \"A piece of advice\"\n",
        "cat(generate(prompt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Compiled generation function for the mini-GPT model\n",
        "compiled_generate <- function(prompt, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  tokens[seq(prompt_length + 1, max_length)] <- 0L                              # <1>\n",
        "  dim(tokens) <- c(1, max_length)\n",
        "  storage.mode(tokens) <- \"integer\"\n",
        "  for (i in seq(prompt_length, max_length - 1)) {\n",
        "    prediction <- mini_gpt |> predict(tokens, verbose = 0)\n",
        "    prediction <- prediction[, i, ]\n",
        "    next_token <- which.max(prediction) - 1L\n",
        "    tokens[, i + 1] <- next_token\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "system.time(compiled_generate(prompt, 64))[[\"elapsed\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate <- function(prompt, sample_fn, max_length = 64) {\n",
        "  tokens <- as.array(tokenizer(prompt))\n",
        "  prompt_length <- length(tokens)\n",
        "  tokens[seq(prompt_length + 1, max_length)] <- 0L\n",
        "  dim(tokens) <- c(1, max_length)\n",
        "  storage.mode(tokens) <- \"integer\"\n",
        "  for (i in seq(prompt_length, max_length - 1)) {\n",
        "    prediction <- predict(mini_gpt, tokens, verbose = 0)\n",
        "    prediction <- prediction[, i, ]\n",
        "    next_token <- sample_fn(prediction) - 1L\n",
        "    tokens[, i + 1] <- as.array(next_token)\n",
        "  }\n",
        "  tokenizer$detokenize(tokens)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hide\n",
        "greedy_search <- function(preds) {\n",
        "  op_argmax(preds)\n",
        "}\n",
        "\n",
        "compiled_generate(prompt, greedy_search)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "random_sample <- function(preds, temperature = 1) {\n",
        "  preds <- preds / temperature\n",
        "  preds <- op_reshape(preds, c(1, -1))\n",
        "  random_categorical(preds, num_samples = 1) |> op_squeeze()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "cat(compiled_generate(prompt, random_sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(x) random_sample(x, temperature = 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(x) random_sample(x, temperature = 0.8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(x) random_sample(x, temperature = 0.2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| results: hide\n",
        "top_k <- function(preds, k = 5, temperature = 1) {\n",
        "  preds <-  preds / temperature\n",
        "  .[top_preds, top_indices] <- op_top_k(preds, k = k, sorted = FALSE)\n",
        "  choice <- random_sample(top_preds)\n",
        "  op_take(top_indices, choice)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(preds) top_k(preds, k = 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(preds) top_k(preds, k = 20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "compiled_generate(prompt, \\(preds) top_k(preds, k = 5, temperature=0.5))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
