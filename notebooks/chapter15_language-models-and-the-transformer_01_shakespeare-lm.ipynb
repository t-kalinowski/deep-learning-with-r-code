{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "# Install required R packages (if needed)\n",
        "pkgs <- c(\"keras3\", \"stringr\", \"tfdatasets\")\n",
        "to_install <- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\n",
        "if (length(to_install)) install.packages(to_install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "Sys.setenv(\"KERAS_BACKEND\"=\"jax\")\n",
        "library(tfdatasets, exclude = c(\"shape\"))\n",
        "library(stringr)\n",
        "library(keras3)\n",
        "reticulate::py_require(\"keras-hub\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Downloading some of Shakespeare's work\"\n",
        "library(keras3)\n",
        "\n",
        "filename = get_file(origin = paste0(\n",
        "  \"https://storage.googleapis.com/download.tensorflow.org/\",\n",
        "  \"data/shakespeare.txt\"\n",
        "))\n",
        "shakespeare <- readLines(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "writeLines(head(shakespeare, 14))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Splitting text into chunks for language model training\n",
        "sequence_length <- 100                                                          # <1>\n",
        "\n",
        "split_input <- function(text, sequence_length) {\n",
        "  starts <- seq.int(1, str_length(text), by = sequence_length)\n",
        "  str_sub(text, cbind(starts, length = sequence_length))\n",
        "}\n",
        "\n",
        "shakespeare <- shakespeare |> str_flatten(\"\\n\")\n",
        "features <- shakespeare |> str_sub(end = -2) |> split_input(sequence_length)\n",
        "labels <- shakespeare |> str_sub(start = 2) |> split_input(sequence_length)\n",
        "\n",
        "dataset <- tensor_slices_dataset(tuple(features, labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "dataset |>\n",
        "  as_iterator() |> iter_next() |>\n",
        "  lapply(tf$strings$substr, 0L, len = 20L)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: \"Learning a character-level vocabulary with `TextVectorization`\"\n",
        "tokenizer <- layer_text_vectorization(\n",
        "  standardize = NULL,\n",
        "  split = \"character\",\n",
        "  output_sequence_length = sequence_length\n",
        ")\n",
        "features_only_dataset <- dataset |> dataset_map(\\(text, labels) text)\n",
        "adapt(tokenizer, features_only_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "vocabulary_size <- tokenizer$vocabulary_size()\n",
        "vocabulary_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "dataset <- dataset |>\n",
        "  dataset_map(\\(features, labels) {\n",
        "    tuple(tokenizer(features), tokenizer(labels))\n",
        "  }, num_parallel_calls = 8)\n",
        "\n",
        "training_data <-  dataset |>\n",
        "  dataset_cache() |>\n",
        "  dataset_shuffle(10000) |>\n",
        "  dataset_batch(64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Building a miniature language model\n",
        "embedding_dim <- 256L\n",
        "hidden_dim <- 1024L\n",
        "\n",
        "inputs <- keras_input(shape = c(sequence_length), dtype = \"int\",\n",
        "                      name = \"token_ids\")\n",
        "\n",
        "outputs <- inputs |>\n",
        "  layer_embedding(vocabulary_size, embedding_dim) |>\n",
        "  layer_gru(hidden_dim, return_sequences = TRUE) |>\n",
        "  layer_dropout(0.1) |>\n",
        "  layer_dense(vocabulary_size, activation = \"softmax\")                          # <1>\n",
        "\n",
        "model <- keras_model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Training a miniature language model\n",
        "model |> compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = \"sparse_categorical_crossentropy\",\n",
        "  metrics = \"sparse_categorical_accuracy\"\n",
        ")\n",
        "model |> fit(training_data, epochs = 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Modifying the language model for autoregressive inference\n",
        "inputs <- keras_input(shape = c(1), dtype = \"int\", name = \"token_ids\")          # <1>\n",
        "input_state <- keras_input(shape = c(hidden_dim), name = \"state\")\n",
        "gru <- layer_gru(units = hidden_dim, return_state = TRUE)\n",
        "\n",
        "x <- inputs |> layer_embedding(vocabulary_size, embedding_dim)\n",
        ".[x, output_state] <- gru(x, initial_state = input_state)\n",
        "outputs <- x |> layer_dense(vocabulary_size, activation=\"softmax\")\n",
        "generation_model <- keras_model(\n",
        "  inputs = list(inputs, input_state),\n",
        "  outputs = list(outputs, output_state)\n",
        ")\n",
        "set_weights(generation_model, get_weights(model))                               # <2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "vocab <- get_vocabulary(tokenizer)\n",
        "\n",
        "chars_to_ids <- \\(chars) match(chars, vocab, nomatch = 2L) - 1L                 # <1>\n",
        "ids_to_chars <- \\(ids) vocab[ids + 1L]                                          # <1>\n",
        "\n",
        "prompt <- r\"--(\n",
        "KING RICHARD III:\n",
        ")--\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Computing a language modelâ€™s starting state\n",
        "input_ids <- chars_to_ids(str_split_1(prompt, \"\"))\n",
        "state <- op_zeros(shape = c(1, hidden_dim))\n",
        "for (token_id in input_ids) {\n",
        "  inputs <- op_expand_dims(token_id, axis = 1)\n",
        "  .[predictions, state] <- generation_model(tuple(inputs, state))              # <1>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "#| lst-cap: Predicting with the language model a token at a time\n",
        "max_length <- 250\n",
        "generated_ids <- integer(max_length)\n",
        "\n",
        "for (i in seq_len(max_length)) {                                                # <1>\n",
        "  next_char_id <- op_argmax(predictions, axis = -1,                             # <2>\n",
        "                            zero_indexed = TRUE, keepdims = TRUE)\n",
        "  generated_ids[i] <- as.array(next_char_id)\n",
        "  .[predictions, state] <- generation_model(list(next_char_id, state))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": [],
      "outputs": [],
      "source": [
        "output <- generated_ids |> ids_to_chars() |> str_flatten(\"\")\n",
        "writeLines(c(prompt, output))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "version": "4.5.2",
      "mimetype": "text/x-r-source",
      "codemirror_mode": {
        "name": "r",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
